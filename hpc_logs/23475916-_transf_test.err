Loaded module: cuda/11.8
2024-12-18 18:01:14.393 | INFO     | fault_management_uds.config:<module>:15 - PROJ_ROOT path is: /work3/s194262/GitHub/fault_management_uds
2024-12-18 18:01:16.658002: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-12-18 18:01:16.750352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1734541276.795924 4057232 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1734541276.810072 4057232 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-18 18:01:16.899890: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Seed set to 42
Cross-validation:   0%|          | 0/1 [00:00<?, ?it/s]GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /work3/s194262/GitHub/fault_management_uds/models/lstm/rain_event_priority=100_241218_1801/1_split exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type      | Params | Mode 
--------------------------------------------
0 | model | LSTMModel | 17.5 K | train
--------------------------------------------
17.5 K    Trainable params
0         Non-trainable params
17.5 K    Total params
0.070     Total estimated model params size (MB)
4         Modules in train mode
0         Modules in eval mode
/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.
Cross-validation:   0%|          | 0/1 [03:04<?, ?it/s]
Traceback (most recent call last):
  File "/work3/s194262/GitHub/fault_management_uds/fault_management_uds/main.py", line 207, in <module>
    main()
  File "/work3/s194262/GitHub/fault_management_uds/fault_management_uds/main.py", line 157, in main
    model, callbacks, logger = train_model(model, train_loader, val_loader, callbacks, logger, config.config['training_args'], current_save_folder)
  File "/work3/s194262/GitHub/fault_management_uds/fault_management_uds/modelling/train.py", line 38, in train_model
    trainer.fit(model, train_loader, val_loader)
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 278, in advance
    trainer._logger_connector.update_train_step_metrics()
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py", line 163, in update_train_step_metrics
    self.log_metrics(self.metrics["log"])
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py", line 118, in log_metrics
    logger.save()
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/lightning_utilities/core/rank_zero.py", line 42, in wrapped_fn
    return fn(*args, **kwargs)
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/loggers/tensorboard.py", line 218, in save
    save_hparams_to_yaml(hparams_file, self.hparams)
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/pytorch_lightning/core/saving.py", line 369, in save_hparams_to_yaml
    with fs.open(config_yaml, "w", newline="") as fp:
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/fsspec/spec.py", line 1289, in open
    self.open(
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/fsspec/spec.py", line 1301, in open
    f = self._open(
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/fsspec/implementations/local.py", line 195, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/fsspec/implementations/local.py", line 359, in __init__
    self._open()
  File "/work3/s194262/thesis/lib64/python3.9/site-packages/fsspec/implementations/local.py", line 364, in _open
    self.f = open(self.path, mode=self.mode)
OSError: [Errno 121] Remote I/O error: '/work3/s194262/GitHub/fault_management_uds/models/lstm/rain_event_priority=100_241218_1801/1_split/version_0/hparams.yaml'
