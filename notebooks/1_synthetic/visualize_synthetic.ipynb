{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "\n",
    "- For each sensor\n",
    "    - For wet and dry periods\n",
    "        - Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 10:48:32.431 | INFO     | fault_management_uds.config:<module>:11 - PROJ_ROOT path is: /Users/arond.jacobsen/Documents/GitHub/fault_management_uds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import nexusformat.nexus as nx\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from fault_management_uds.data.hdf_functions import print_tree, load_dataframe_from_HDF5\n",
    "from fault_management_uds.data.process import remove_nans_from_start_end\n",
    "\n",
    "from fault_management_uds.modelling.classifiers import classify_rain_events\n",
    "from fault_management_uds.plots import get_segment_start_end_color, set_meaningful_xticks\n",
    "from fault_management_uds.plots import visualize_error_span\n",
    "from fault_management_uds.config import indicator_2_meta, bools_2_meta, error_indicators, natural_sensor_order\n",
    "\n",
    "\n",
    "from fault_management_uds.config import PROJ_ROOT\n",
    "from fault_management_uds.config import DATA_DIR, RAW_DATA_DIR, INTERIM_DATA_DIR, PROCESSED_DATA_DIR, EXTERNAL_DATA_DIR\n",
    "from fault_management_uds.config import MODELS_DIR, REPORTS_DIR, FIGURES_DIR, REFERENCE_DIR\n",
    "\n",
    "\n",
    "from fault_management_uds.data.load import import_external_metadata, import_metadata\n",
    "from fault_management_uds.data.load import load_data_period, filenames_based_on_period, provided_2_full_range, get_event\n",
    "from fault_management_uds.data.format import create_individual_indicators, create_indicator\n",
    "\n",
    "\n",
    "from fault_management_uds.synthetic.synthetic_generator import AnomalyHandler\n",
    "\n",
    "\n",
    "# set random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = PROCESSED_DATA_DIR / 'Bellinge.h5'\n",
    "external_metadata = import_metadata(REFERENCE_DIR / 'external_metadata.csv')\n",
    "metadata = import_metadata(REFERENCE_DIR / 'sensor_metadata.csv')\n",
    "\n",
    "# Raw sensor path\n",
    "raw_sensor_path = RAW_DATA_DIR / 'Bellinge' / 'sensor-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure save folder\n",
    "figure_save_folder = FIGURES_DIR / 'synthetic'\n",
    "figure_save_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dry and wet periods for each sensor\n",
    "\n",
    "- We want non-erroneous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the classified rain events\n",
    "# clf_rain_events = pd.read_csv(REFERENCE_DIR / 'evetns' / 'rain_events.csv', index_col=0)\n",
    "# clf_rain_events['start'] = pd.to_datetime(clf_rain_events['start'])\n",
    "# clf_rain_events['end'] = pd.to_datetime(clf_rain_events['end'])\n",
    "\n",
    "# sub_rain_events = clf_rain_events[clf_rain_events['duration'] < 60].copy()\n",
    "# # sort by total rain\n",
    "# sub_rain_events.sort_values('total_rain', ascending=False, inplace=True)\n",
    "# sub_rain_events.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dry_wet_periods = {}\n",
    "\n",
    "# # create a dry event dataframe that goes from rain event end to next rain event start\n",
    "# dry_events = []\n",
    "# # sort clf_rain_events by start\n",
    "# clf_rain_events.sort_values('start', inplace=True)\n",
    "# for i in range(clf_rain_events.shape[0] - 1):\n",
    "#     end = clf_rain_events.loc[i, 'end']\n",
    "#     start = clf_rain_events.loc[i+1, 'start']\n",
    "#     duration = (start - end).total_seconds() / 60\n",
    "#     dry_events.append({'start': end, 'end': start, 'duration': duration})\n",
    "\n",
    "# dry_events = pd.DataFrame(dry_events)\n",
    "# # sort by duration\n",
    "# dry_events.sort_values('duration', ascending=False, inplace=True)\n",
    "# dry_events.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# for sensor_name in natural_sensor_order:\n",
    "#     print(f\"Sensor: {sensor_name}\")\n",
    "#     dry_wet_periods[sensor_name] = {}\n",
    "#     # load the data\n",
    "#     data, _, _, _ = load_dataframe_from_HDF5(data_file_path, f\"single_series/sewer_data/{sensor_name}/{sensor_name}_clean\")\n",
    "#     data = remove_nans_from_start_end(data, 'value')\n",
    "    \n",
    "    \n",
    "#     ### Find a rain event\n",
    "#     dry_wet_periods[sensor_name]['rain_event'] = {}\n",
    "#     # filter rain events given sensor time range\n",
    "#     start, end = data.index[0], data.index[-1]\n",
    "#     sensor_rain_events = sub_rain_events[(sub_rain_events['start'] < end) & (sub_rain_events['end'] > start)].copy()\n",
    "#     # iterate rain events until one is found, where the data is not missing\n",
    "#     for i, rain_event in sensor_rain_events.iterrows():\n",
    "#         # get the data for the rain event\n",
    "#         data_rain_period = data.loc[rain_event['start']:rain_event['end']]\n",
    "#         # check if the data is missing\n",
    "#         if data_rain_period['value'].isna().sum() == 0:\n",
    "#             print(f\"    Found rain event on attempt {i+1}\")\n",
    "#             # round up and down to nearest half hour\n",
    "#             dry_wet_periods[sensor_name]['rain_event']['start'] = rain_event['start'].replace(minute=0, second=0, microsecond=0)\n",
    "#             dry_wet_periods[sensor_name]['rain_event']['end'] = rain_event['start'].replace(minute=0, second=0, microsecond=0) + timedelta(hours=2)\n",
    "#             break\n",
    "#     # this else will only be executed if the for loop is not broken\n",
    "#     else:\n",
    "#         print(\"    No rain event found\")\n",
    "#         dry_wet_periods[sensor_name]['rain_event']['start'] = None\n",
    "#         dry_wet_periods[sensor_name]['rain_event']['end'] = None\n",
    "\n",
    "\n",
    "#     ### Find a dry period\n",
    "#     dry_wet_periods[sensor_name]['dry_period'] = {}\n",
    "#     # filter out dry events given sensor time range\n",
    "#     sensor_dry_events = dry_events[(dry_events['start'] < end) & (dry_events['end'] > start)].copy()\n",
    "#     # iterate dry events until one is found, where the data is not missing\n",
    "#     for i, dry_event in sensor_dry_events.iterrows():\n",
    "#         # get the data for the dry event\n",
    "#         data_dry_period = data.loc[dry_event['start']:dry_event['end']]\n",
    "#         # check if the data is missing\n",
    "#         if data_dry_period['value'].isna().sum() == 0:\n",
    "#             print(f\"    Found dry event on attempt {i+1}\")\n",
    "#             dry_wet_periods[sensor_name]['dry_period']['start'] = dry_event['start'].replace(minute=0, second=0, microsecond=0)\n",
    "#             dry_wet_periods[sensor_name]['dry_period']['end'] = dry_event['start'].replace(minute=0, second=0, microsecond=0) + timedelta(hours=2)\n",
    "#             break\n",
    "\n",
    "#     # this else will only be executed if the for loop is not broken\n",
    "#     else:\n",
    "#         print(\"    No dry event found\")\n",
    "#         dry_wet_periods[sensor_name]['dry_period']['start'] = None\n",
    "#         dry_wet_periods[sensor_name]['dry_period']['end'] = None\n",
    "\n",
    "# del data\n",
    "# # save the sensor dry wet periods\n",
    "# with open(REFERENCE_DIR / 'events' / 'dry_wet_periods.json', 'w') as f:\n",
    "#     json.dump(dry_wet_periods, f, default=str, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dry and wet periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sensor dry wet periods\n",
    "with open(REFERENCE_DIR / 'events' / 'dry_wet_periods.json', 'r') as f:\n",
    "    dry_wet_periods = json.load(f)\n",
    "    # convert the start and end times to datetime\n",
    "    for sensor_name in dry_wet_periods.keys():\n",
    "        for event_type in dry_wet_periods[sensor_name].keys():\n",
    "            for event in dry_wet_periods[sensor_name][event_type].keys():\n",
    "                dry_wet_periods[sensor_name][event_type][event] = pd.to_datetime(dry_wet_periods[sensor_name][event_type][event])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sensor ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sensor ranges\n",
    "with open(REFERENCE_DIR / 'sensor_ranges.json', 'r') as f:\n",
    "    sensor_ranges = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml file\n",
    "with open(REFERENCE_DIR / 'synthetic_config.yaml', 'r') as f:\n",
    "    synthetic_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualize anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(sensor_name, period_type, dry_wet_periods, buffer=None):\n",
    "\n",
    "    # get the start and end times\n",
    "    start = dry_wet_periods[sensor_name][period_type]['start']\n",
    "    end = dry_wet_periods[sensor_name][period_type]['end']\n",
    "\n",
    "    if buffer == \"day\":\n",
    "        # round start to start of day and end to end of day\n",
    "        start = start.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        # end is then the next day\n",
    "        end = start + timedelta(days=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # load the event data\n",
    "    rain_5425, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5425\", starttime=start, endtime=end, complete_range=True, verbose=True)\n",
    "    rain_5427, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5427\", starttime=start, endtime=end, complete_range=True, verbose=True)\n",
    "    sensor_data, _, _, _ = load_dataframe_from_HDF5(data_file_path, f\"single_series/sewer_data/{sensor_name}/clean\", starttime=start, endtime=end, complete_range=True, verbose=True)\n",
    "\n",
    "\n",
    "    data_dict = {\n",
    "        '5425': rain_5425,\n",
    "        '5427': rain_5427,\n",
    "        'original': sensor_data,\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "    }\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n",
    "def insert_anomaly(data_dict, anomaly_config, anomaly, sensor_scale, obvious_min, obvious_max, seed, center=False):\n",
    "    # create an anomaly handler\n",
    "    n_obs = data_dict['original'].shape[0]\n",
    "    anomaly_handler = AnomalyHandler(anomaly_config, anomaly, 'value', n_obs, sensor_scale, obvious_min, obvious_max, seed)\n",
    "    \n",
    "    # handling these example cases\n",
    "    # these can have multiple injections\n",
    "    if center:\n",
    "        # set start index to be in the middle of the time series\n",
    "        start_idx = n_obs*2 // 5\n",
    "        anomaly_handler.set_injection_start(start_idx)\n",
    "    else:\n",
    "        anomaly_handler.initialize_injections()\n",
    "\n",
    "    polluted_sensor = anomaly_handler.inject_anomalies(data_dict['original'])\n",
    "    data_dict['polluted'] = polluted_sensor\n",
    "    data_dict['indicator_dict'] = {\n",
    "        'indicator': anomaly_handler.get_indicator(),\n",
    "        'colormap': {\n",
    "            0: 'none',\n",
    "            1: 'firebrick',\n",
    "        }\n",
    "    }\n",
    "    return data_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rain(ax, title, data_dict, marker, linewidth=1):\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ### Visualize rain data\n",
    "    for rain_gauge, rain_color in zip(['5427', '5425'], ['purple', 'darkblue']):\n",
    "        ax.plot(data_dict[rain_gauge].index, data_dict[rain_gauge].value, \n",
    "            label=f'Rain gauge {rain_gauge}', color=rain_color,\n",
    "            linewidth=linewidth, linestyle='-', \n",
    "            marker=marker, markersize=1, alpha=1)\n",
    "        ax.set_ylabel('Rain (mm)')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.set_xticks([])\n",
    "        # set y limits based on 0 and max wrt both\n",
    "        ax.set_ylim(-1, data_dict['max_rain'])  \n",
    "        ax.set_xlim(data_dict['start'], data_dict['end'])\n",
    "    return ax\n",
    "\n",
    "\n",
    "def visualize_injected_synthetics(ax, title, data_dict, unit, marker):\n",
    "\n",
    "    # visualize the sensor data; severities\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.plot(data_dict['original'].index, data_dict['original'].value, \n",
    "        label='Original data', color='grey', \n",
    "        linewidth=1, linestyle='-', \n",
    "        marker='', markersize=1, alpha=1)\n",
    "\n",
    "    ax.plot(data_dict['polluted'].index, data_dict['polluted'].value, \n",
    "        label='Erroneous data', color='grey', \n",
    "        linewidth=2, linestyle='-', \n",
    "        marker=marker, markersize=2, alpha=1)\n",
    "    # visualzie error span\n",
    "    ax = visualize_error_span(ax, data_dict['indicator_dict'], data_dict['start'], data_dict['end'], adjust='full-point')\n",
    "    ax.set_xlim(data_dict['start'], data_dict['end'])\n",
    "    ax.set_ylabel(unit)\n",
    "    ax.legend()\n",
    "    ax.set_xticks([])\n",
    "    return ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder\n",
    "examples_save_folder = figure_save_folder / 'examples'\n",
    "examples_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "anomalies = list(synthetic_config['anomalies'].keys())\n",
    "severity = \"medium\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:22<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sensor_name in tqdm(natural_sensor_order, total=len(natural_sensor_order)):\n",
    "    if sensor_name != 'G80F11B_Level1':\n",
    "        continue\n",
    "\n",
    "    # create a folder for the sensor\n",
    "    sensor_save_folder = examples_save_folder / sensor_name\n",
    "    sensor_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    # extract meta\n",
    "    sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "    sensor_meta = metadata[metadata['IdMeasurement'] == sensor_name]\n",
    "    unit = sensor_meta['UnitAlias'].values[0]\n",
    "    obvious_min = sensor_meta['obvious_min'].values[0]\n",
    "    obvious_max = sensor_meta['obvious_max'].values[0]  \n",
    "\n",
    "\n",
    "    # plot a short and long period\n",
    "    #for buffer in [0, 1080]:\n",
    "    for buffer in ['period', 'day']:\n",
    "        marker = \"o\" if buffer == 'period' else \"\"\n",
    "\n",
    "        dry_data_dict = get_data_dict(sensor_name, 'dry_period', dry_wet_periods, buffer=buffer)\n",
    "        wet_data_dict = get_data_dict(sensor_name, 'rain_event', dry_wet_periods, buffer=buffer)\n",
    "\n",
    "        abs_max_rain = max([dry_data_dict['5425'].value.max(), dry_data_dict['5427'].value.max(), wet_data_dict['5425'].value.max(), wet_data_dict['5427'].value.max()]) + 1\n",
    "        dry_data_dict['max_rain'] = abs_max_rain    \n",
    "        wet_data_dict['max_rain'] = abs_max_rain\n",
    "\n",
    "        ### Visualizing all anomalies in one\n",
    "        fig, axs = plt.subplots(1+len(anomalies), 2, figsize=(14, 14), sharey=\"row\")\n",
    "        # set main title\n",
    "        fig.suptitle(f\"{sensor_name}\\n{buffer.capitalize()}\", fontsize=24)\n",
    "        axs[0, 0] = visualize_rain(axs[0, 0], 'Dry period', dry_data_dict, marker)\n",
    "        axs[0, 1] = visualize_rain(axs[0, 1], 'Rain event', wet_data_dict, marker)\n",
    "        # iterate over the anomalies\n",
    "        for i, anomaly in enumerate(anomalies):\n",
    "            anomaly_config = synthetic_config['anomalies'][anomaly][severity]\n",
    "\n",
    "            dry_data_dict = insert_anomaly(dry_data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=True)\n",
    "            wet_data_dict = insert_anomaly(wet_data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=True)\n",
    "            \n",
    "            # Visualize the injected anomalies\n",
    "            sensor_title = f\"{anomaly.capitalize()}\"\n",
    "            axs[i+1, 0] = visualize_injected_synthetics(axs[i+1, 0], sensor_title, dry_data_dict, unit, marker)\n",
    "            axs[i+1, 1] = visualize_injected_synthetics(axs[i+1, 1], sensor_title, wet_data_dict, unit, marker)\n",
    "            \n",
    "\n",
    "        axs[-1, 0] = set_meaningful_xticks(axs[-1, 0], dry_data_dict['start'], dry_data_dict['end'])\n",
    "        axs[-1, 1] = set_meaningful_xticks(axs[-1, 1], wet_data_dict['start'], wet_data_dict['end'])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # # save the figure\n",
    "        fig.savefig(sensor_save_folder / f\"all_{buffer}.png\", dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "        continue\n",
    "\n",
    "\n",
    "        ### Visualizing each anomaly\n",
    "        # iterate over the anomalies\n",
    "        for anomaly in anomalies:\n",
    "            anomaly_config = synthetic_config['anomalies'][anomaly][severity]\n",
    "            # using standard deviation as the range\n",
    "            #sensor_range = sensor_ranges[sensor_name]['clean']['std']\n",
    "            sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "\n",
    "\n",
    "            dry_data_dict = insert_anomaly(dry_data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=True)\n",
    "            wet_data_dict = insert_anomaly(wet_data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=True)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(16, 6), sharey=\"row\", height_ratios=[1, 2])\n",
    "            # set main title\n",
    "            fig.suptitle(f\"{sensor_name}\\n{anomaly.capitalize()}\", fontsize=24)\n",
    "            sensor_title = f\"{severity.capitalize()} severity\"\n",
    "            axs[0, 0] = visualize_rain(axs[0, 0], 'Dry period', dry_data_dict)\n",
    "            axs[1, 0] = visualize_injected_synthetics(axs[1, 0], sensor_title, dry_data_dict, unit)\n",
    "            axs[-1, 0] = set_meaningful_xticks(axs[-1, 0], dry_data_dict['start'], dry_data_dict['end'])\n",
    "\n",
    "            axs[0, 1] = visualize_rain(axs[0, 1], 'Rain event', wet_data_dict)\n",
    "            axs[1, 1] = visualize_injected_synthetics(axs[1, 1], sensor_title, wet_data_dict, unit)\n",
    "            axs[-1, 1] = set_meaningful_xticks(axs[-1, 1], wet_data_dict['start'], wet_data_dict['end'])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            # save the figure\n",
    "            fig.savefig(sensor_save_folder / f\"{buffer}_{anomaly}.png\", dpi=150)\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize full timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_name = 'G80F11B_Level1'\n",
    "unit = metadata[metadata['IdMeasurement'] == sensor_name]['UnitAlias'].values[0]\n",
    "\n",
    "# create a folder for the sensor\n",
    "sensor_save_folder = examples_save_folder / sensor_name\n",
    "sensor_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# extract meta\n",
    "sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "sensor_meta = metadata[metadata['IdMeasurement'] == sensor_name]\n",
    "unit = sensor_meta['UnitAlias'].values[0]\n",
    "obvious_min = sensor_meta['obvious_min'].values[0]\n",
    "obvious_max = sensor_meta['obvious_max'].values[0]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full data\n",
    "sensor_data, _, _, _ = load_dataframe_from_HDF5(data_file_path, f\"single_series/sewer_data/{sensor_name}/clean\")\n",
    "\n",
    "start, end = sensor_data.index[0], sensor_data.index[-1]\n",
    "\n",
    "# load the event data\n",
    "rain_5425, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5425\", starttime=start, endtime=end, complete_range=True)\n",
    "rain_5427, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5427\", starttime=start, endtime=end, complete_range=True)\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    '5425': rain_5425,\n",
    "    '5427': rain_5427,\n",
    "    'original': sensor_data,\n",
    "    'start': start,\n",
    "    'end': end,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_injected_synthetics(ax, title, data_dict, unit, marker):\n",
    "\n",
    "    # visualize the sensor data; severities\n",
    "    ax.set_title(title, fontsize=20)\n",
    "\n",
    "    ax.plot(data_dict['polluted'].index, data_dict['polluted'].value, \n",
    "        label='Erroneous data', color='grey', \n",
    "        linewidth=0.3, linestyle='-', \n",
    "        marker=marker, markersize=1, alpha=1)\n",
    "    # visualzie error span\n",
    "    ax = visualize_error_span(ax, data_dict['indicator_dict'], data_dict['start'], data_dict['end'], adjust=60*3, alpha=0.5)\n",
    "    ax.set_xlim(data_dict['start'], data_dict['end'])\n",
    "    ax.set_ylabel(unit)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xticks([])\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain done\n",
      "spike done: 0.00010136105958880849\n",
      "noise done: 0.0020669296481099298\n",
      "frozen done: 0.005003265498053763\n",
      "offset done: 0.020494788265106194\n",
      "drift done: 0.020130097442461924\n"
     ]
    }
   ],
   "source": [
    "marker = \"\"\n",
    "\n",
    "abs_max_rain = max([data_dict['5425'].value.max(), data_dict['5427'].value.max()]) + 1\n",
    "data_dict['max_rain'] = abs_max_rain    \n",
    "\n",
    "### Visualizing all anomalies in one\n",
    "fig, axs = plt.subplots(1+len(anomalies), 1, figsize=(14, 14), sharey=\"row\", dpi=150)\n",
    "# set main title\n",
    "fig.suptitle(f\"{sensor_name}\\n{' '}\", fontsize=24)\n",
    "axs[0] = visualize_rain(axs[0], 'Rain', data_dict, marker, linewidth=0.3)\n",
    "print('Rain done')\n",
    "# iterate over the anomalies\n",
    "for i, anomaly in enumerate(anomalies):\n",
    "    anomaly_config = synthetic_config['anomalies'][anomaly][severity]\n",
    "    sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "\n",
    "    data_dict = insert_anomaly(data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=False)\n",
    "    \n",
    "    # Visualize the injected anomalies\n",
    "    sensor_title = f\"{anomaly.capitalize()}\"\n",
    "    axs[i+1] = visualize_injected_synthetics(axs[i+1], sensor_title, data_dict, unit, marker)\n",
    "\n",
    "    print(f\"{anomaly} done: {data_dict['indicator_dict']['indicator'].sum() / data_dict['indicator_dict']['indicator'].shape[0]}\")\n",
    "\n",
    "    \n",
    "\n",
    "axs[-1] = set_meaningful_xticks(axs[-1], data_dict['start'], data_dict['end'])\n",
    "plt.tight_layout()\n",
    "# # save the figure\n",
    "fig.savefig(sensor_save_folder / f\"all_complete.png\", dpi=150)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Todo\n",
    "\n",
    "- compare this real errors?\n",
    "- some synthetic data summary?\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
