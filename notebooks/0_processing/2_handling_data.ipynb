{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this script we'll create two files:\n",
    "\n",
    "- single series file: contains each series (sensor, rain gauge) with additional metadata\n",
    "- combined file: contains all the series combined based on timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 20:42:41.277 | INFO     | fault_management_uds.config:<module>:11 - PROJ_ROOT path is: /Users/arond.jacobsen/Documents/GitHub/fault_management_uds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import nexusformat.nexus as nx\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "\n",
    "from fault_management_uds.data.hdf_tools import delete_group, create_group, print_tree, save_dataframe_in_HDF5, load_dataframe_from_HDF5, update_filtered_data_in_HDF5\n",
    "from fault_management_uds.data.process import ensure_data_is_from_start_to_end, remove_nans_from_start_end\n",
    "from fault_management_uds.data.load import import_external_metadata, import_metadata\n",
    "from fault_management_uds.config import bools_2_meta, error_indicators\n",
    "\n",
    "\n",
    "\n",
    "from fault_management_uds.config import PROJ_ROOT\n",
    "from fault_management_uds.config import DATA_DIR, RAW_DATA_DIR, INTERIM_DATA_DIR, PROCESSED_DATA_DIR, EXTERNAL_DATA_DIR\n",
    "from fault_management_uds.config import MODELS_DIR, REPORTS_DIR, FIGURES_DIR, REFERENCE_DIR\n",
    "from fault_management_uds.config import natural_sensor_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# TODO:\n",
    "\n",
    "- what does the comment; scaling factor wrong initially mean?\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data\n",
    "\n",
    "check the bellinge article, \n",
    "\n",
    "- Figure 4: how the sewage system works and what the sensors measure\n",
    "- Table 1: the sensors\n",
    "- Figure 7: the sewer data, when available, when outliers, when nan\n",
    "- Figure 13: The available data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = PROCESSED_DATA_DIR / 'Bellinge.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 file created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mode = 'w' if not os.path.exists(data_file_path) else 'a'\n",
    "# Open or create an HDF5 file in append mode\n",
    "with h5py.File(data_file_path, mode) as hdf:\n",
    "    pass\n",
    "\n",
    "print(\"HDF5 file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data will contain two groups\n",
    "single_series_group = \"single_series\"\n",
    "create_group(data_file_path, single_series_group, verbose=False)\n",
    "# single series will be groups by series type, e.g. rain gauge, sewer data..\n",
    "\n",
    "combined_data_group = \"combined_data\"\n",
    "create_group(data_file_path, combined_data_group, verbose=False)\n",
    "# combined data will combine each series by date and contain raw, cleaned, indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "├── combined_data\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "└── single_series\n",
      "    ├── rain_gauge_data\n",
      "    │   ├── 5425\n",
      "    │   │   ├── columns\n",
      "    │   │   ├── data\n",
      "    │   │   └── timestamps\n",
      "    │   └── 5427\n",
      "    │       ├── columns\n",
      "    │       ├── data\n",
      "    │       └── timestamps\n",
      "    └── sewer_data\n",
      "        ├── G71F04R_Level1\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G71F04R_Level2\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G71F05R_LevelBasin\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G71F05R_LevelInlet\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G71F05R_position\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G71F06R_LevelInlet\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G71F68Y_LevelPS\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G71F68Yp1\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G71F68Yp1_power\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G71F68Yp2_power\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G72F040\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G73F010\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G80F11B_Level1\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G80F11B_Level2\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G80F13P_LevelPS\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G80F13Pp1_power\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G80F13Pp2_power\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        ├── G80F66Y_Level1\n",
      "        │   ├── bools\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   ├── clean\n",
      "        │   │   ├── columns\n",
      "        │   │   ├── data\n",
      "        │   │   └── timestamps\n",
      "        │   └── raw\n",
      "        │       ├── columns\n",
      "        │       ├── data\n",
      "        │       └── timestamps\n",
      "        └── G80F66Y_Level2\n",
      "            ├── bools\n",
      "            │   ├── columns\n",
      "            │   ├── data\n",
      "            │   └── timestamps\n",
      "            ├── clean\n",
      "            │   ├── columns\n",
      "            │   ├── data\n",
      "            │   └── timestamps\n",
      "            └── raw\n",
      "                ├── columns\n",
      "                ├── data\n",
      "                └── timestamps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NXroot('Bellinge')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_tree(data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Single Series Data\n",
    "\n",
    "missing:\n",
    "- temperature\n",
    "- band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sensor data\n",
    "\n",
    "Goal:\n",
    "- for each sensor, combine its multiple files into a single series\n",
    "- quicker and selective loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['G71F68Y_LevelPS_System2000_p1.pkl', 'G71F68Yp1_System2000_p1.pkl', 'G71F68Yp1_power_System2000_p1.pkl', 'G71F05R_LevelBasin_System2000_p1.pkl', 'G71F68Yp1_power_iFix_p1.pkl', 'G71F05R_LevelBasin_iFix_p1.pkl', 'G80F11B_Level2_iFix_p1.pkl', 'G71F05R_LevelInlet_System2000_p1.pkl', 'G72F040_Danova_p1.pkl', 'G80F66Y_Level1_iFix_p1.pkl', 'G80F66Y_Level2_iFix_p1.pkl', 'G80F11B_Level1_iFix_p1.pkl', 'G71F68Y_LevelPS_iFix_p1.pkl', 'G71F68Yp2_power_System2000_p1.pkl', 'G71F68Yp2_power_iFix_p1.pkl', 'G71F05R_position_iFix_p1.pkl', 'G73F010_Danova_p1.pkl', 'G71F04R_Level2_System2000_p1.pkl', 'G71F04R_Level2_iFix_p4.pkl', 'G71F04R_Level2_System2000_p2.pkl', 'G71F05R_LevelInlet_iFix_p1.pkl', 'G80F13P_LevelPS_iFix_p1.pkl', 'G80F13Pp2_power_iFix_p1.pkl', 'metadata.csv', 'G80F13Pp2_power_System2000_p1.pkl', 'G71F04R_Level2_iFix_p3.pkl', 'G71F68Yp1_iFix_p1.pkl', 'G71F04R_Level1_iFix_p3.pkl', 'G80F13Pp1_power_System2000_p1.pkl', 'G80F13Pp1_power_iFix_p1.pkl', 'G71F06R_LevelInlet_iFix_p1.pkl', 'G71F06R_LevelInlet_System2000_p1.pkl', 'G80F13P_LevelPS_System2000_p1.pkl', 'G71F04R_Level1_System2000_p1.pkl', 'G71F05R_position_System2000_p1.pkl', 'G71F04R_Level1_iFix_p4.pkl', 'G71F04R_Level1_System2000_p2.pkl']\n"
     ]
    }
   ],
   "source": [
    "# get the paths\n",
    "interim_sensor_path = INTERIM_DATA_DIR / 'Bellinge' / 'sensor-data' \n",
    "# list all the files in the folder\n",
    "files = os.listdir(interim_sensor_path)\n",
    "print(len(files))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_metadata = import_metadata(REFERENCE_DIR / 'external_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area: ['Bellinge']\n",
      "\n",
      "Source: ['System2000' 'iFix' 'Danova']\n",
      "\n",
      "Version: ['p1' 'p2' 'p3' 'p4']\n",
      "\n",
      "Type: ['Level' 'Position' 'Discharge' 'Power']\n",
      "\n",
      "StartTime: <DatetimeArray>\n",
      "['2010-08-01 00:00:00', '2020-01-06 00:00:00', '2020-10-12 00:00:00',\n",
      " '2020-11-19 00:00:00', '2020-10-13 00:00:00', '2017-03-07 00:00:00',\n",
      " '2020-01-08 00:00:00', '2019-06-27 00:00:00', '2019-10-23 00:00:00',\n",
      " '2010-01-01 00:00:00', '2018-09-05 00:00:00']\n",
      "Length: 11, dtype: datetime64[ns]\n",
      "\n",
      "EndTime: <DatetimeArray>\n",
      "['2020-01-06 00:00:00', '2020-10-12 00:00:00', '2020-11-19 00:00:00',\n",
      " '2021-08-19 00:00:00', '2020-03-23 00:00:00', '2018-09-04 00:00:00']\n",
      "Length: 6, dtype: datetime64[ns]\n",
      "\n",
      "Conversion: [1.000000e+02 6.896552e+01 6.896552e-01 1.000000e+00 3.600000e+03]\n",
      "\n",
      "comment: ['cm -> m' 'cm -> m + scaling factor wrong initially (2.9/2)'\n",
      " 'scaling factor wrong initially' nan\n",
      " 'cm -> m + scaling factor wrong initially' 'm3/h -> m3/s']\n",
      "\n",
      "unit: ['cm' 'm' 'm3/h' 'A' nan]\n",
      "\n",
      "obvious_min: [ 1.e-03  0.e+00  2.e-03 -5.e+00]\n",
      "\n",
      "obvious_max: [  1.5   5.    3.  ...  50.  100.    4. ]\n",
      "\n",
      "zeropoint: [17.09 17.05 14.14 ... 34.91 33.1  34.23]\n",
      "\n",
      "window_frozen: [ 20  60 360]\n",
      "\n",
      "outlier_threshold: [0.2]\n",
      "\n",
      "outlier_width: [1]\n",
      "\n",
      "SaveName: ['G71F04R_Level1_System2000_p1' 'G71F04R_Level1_System2000_p2'\n",
      " 'G71F04R_Level1_iFix_p3' ... 'G80F13Pp2_power_iFix_p1'\n",
      " 'G80F66Y_Level1_iFix_p1' 'G80F66Y_Level2_iFix_p1']\n",
      "\n",
      "ProvidedSaveName: ['G71F04R_Level1_System2000p1_proc_v6'\n",
      " 'G71F04R_Level1_System2000p2_proc_v6' 'G71F04R_Level1_iFixp3_proc_v6' ...\n",
      " 'G80F13P_LevelPS_iFixp1_proc_v6' 'G80F66Y_Level1_iFixp1_proc_v6'\n",
      " 'G80F66Y_Level2_iFixp1_proc_v6']\n",
      "\n",
      "Unit: ['m' 'm3/s' 'A']\n",
      "\n",
      "UnitAlias: ['Meter' 'm3/s' 'Ampere']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "not_check_unique = ['IdMeasurement', 'Folderpath', 'Filename', 'TagSRO', 'Navn', 'sensor_orientation']\n",
    "for i, col in enumerate(external_metadata.columns):\n",
    "    if col not in not_check_unique:\n",
    "        print(f\"{col}: {external_metadata[col].unique()}\")\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert external_metadata.shape[0] == len(files)-1, f\"metadata shape: {external_metadata.shape[0]}, files: {len(files)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a group if it does not exist\n",
    "sensor_group_path = single_series_group + '/sewer_data'\n",
    "create_group(data_file_path, sensor_group_path, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor name: G71F04R_Level1\n",
      "    File: G71F04R_Level1_System2000_p1\n",
      "    File: G71F04R_Level1_System2000_p2\n",
      "    File: G71F04R_Level1_iFix_p3\n",
      "    File: G71F04R_Level1_iFix_p4\n",
      "    Done\n",
      "\n",
      "Sensor name: G71F04R_Level2\n",
      "    File: G71F04R_Level2_System2000_p1\n",
      "    File: G71F04R_Level2_System2000_p2\n",
      "    File: G71F04R_Level2_iFix_p3\n",
      "    File: G71F04R_Level2_iFix_p4\n",
      "    Done\n",
      "\n",
      "Sensor name: G71F05R_LevelBasin\n",
      "    File: G71F05R_LevelBasin_System2000_p1\n",
      "    File: G71F05R_LevelBasin_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G71F05R_LevelInlet\n",
      "    File: G71F05R_LevelInlet_System2000_p1\n",
      "    File: G71F05R_LevelInlet_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G71F05R_position\n",
      "    File: G71F05R_position_System2000_p1\n",
      "    File: G71F05R_position_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G71F06R_LevelInlet\n",
      "    File: G71F06R_LevelInlet_System2000_p1\n",
      "    File: G71F06R_LevelInlet_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G71F68Y_LevelPS\n",
      "    File: G71F68Y_LevelPS_System2000_p1\n",
      "    File: G71F68Y_LevelPS_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G71F68Yp1\n",
      "    File: G71F68Yp1_System2000_p1\n",
      "    File: G71F68Yp1_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G71F68Yp1_power\n",
      "    File: G71F68Yp1_power_System2000_p1\n",
      "    File: G71F68Yp1_power_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G71F68Yp2_power\n",
      "    File: G71F68Yp2_power_System2000_p1\n",
      "    File: G71F68Yp2_power_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G72F040\n",
      "    File: G72F040_Danova_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G73F010\n",
      "    File: G73F010_Danova_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G80F11B_Level1\n",
      "    File: G80F11B_Level1_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G80F11B_Level2\n",
      "    File: G80F11B_Level2_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G80F13P_LevelPS\n",
      "    File: G80F13P_LevelPS_System2000_p1\n",
      "    File: G80F13P_LevelPS_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G80F13Pp1_power\n",
      "    File: G80F13Pp1_power_System2000_p1\n",
      "    File: G80F13Pp1_power_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G80F13Pp2_power\n",
      "    File: G80F13Pp2_power_System2000_p1\n",
      "    File: G80F13Pp2_power_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G80F66Y_Level1\n",
      "    File: G80F66Y_Level1_iFix_p1\n",
      "    Done\n",
      "\n",
      "Sensor name: G80F66Y_Level2\n",
      "    File: G80F66Y_Level2_iFix_p1\n",
      "    Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the metadata\n",
    "for sensor_name, sensor_metadata in external_metadata.groupby('IdMeasurement'):\n",
    "    # if sensor_name != \"G72F040\":\n",
    "    #     # go to the next sensor\n",
    "    #     continue\n",
    "    print(f\"Sensor name: {sensor_name}\")\n",
    "\n",
    "    # Create a group for the sensor\n",
    "    sensor_path = f\"{sensor_group_path}/{sensor_name}\"\n",
    "    create_group(data_file_path, sensor_path, verbose=False)\n",
    "\n",
    "\n",
    "    # create an empty dataframe to concatenate the data\n",
    "    full_data = pd.DataFrame(columns=['time', 'raw_value', 'value_no_errors'] + error_indicators)\n",
    "    \n",
    "    # Iterate over the rows in the metadata\n",
    "    for i, file_metadata in sensor_metadata.iterrows():\n",
    "        print(f\"    File: {file_metadata['SaveName']}\")\n",
    "        \n",
    "        # Load and prepare\n",
    "        data_path = interim_sensor_path / f\"{file_metadata['SaveName']}.pkl\"\n",
    "        # load the pickle file\n",
    "        with open(data_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        data['time'] = pd.to_datetime(data['time'])\n",
    "        data.sort_values('time', inplace=True)\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        # remove the NaNs from the start and end\n",
    "        data = remove_nans_from_start_end(data, 'raw_value')\n",
    "        # concatenate the data\n",
    "        full_data = pd.concat([full_data, data], axis=0)\n",
    "\n",
    "\n",
    "    # sort values by value_no_errors and drop duplicates\n",
    "    full_data.sort_values(['time', 'value_no_errors', 'raw_value'], inplace=True)\n",
    "    full_data.drop_duplicates(subset='time', keep='first', inplace=True)\n",
    "    full_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    # sort the data by time\n",
    "    full_data.sort_values('time', inplace=True)\n",
    "    full_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save to the HDF5 file\n",
    "    # save the raw\n",
    "    raw_series = full_data[['time', 'raw_value']].copy()\n",
    "    raw_series.columns = ['time', 'value']\n",
    "    save_dataframe_in_HDF5(data_file_path, sensor_path, f\"raw\", raw_series)\n",
    "    del raw_series\n",
    "    # save the clean\n",
    "    clean_series = full_data[['time', 'value_no_errors']].copy()\n",
    "    clean_series.columns = ['time', 'value']\n",
    "    save_dataframe_in_HDF5(data_file_path, sensor_path, f\"clean\", clean_series)\n",
    "    del clean_series\n",
    "\n",
    "    # save the errors\n",
    "    bool_series = full_data[['time'] + ['ffill'] + error_indicators].copy()\n",
    "    # convert to boolean\n",
    "    for col in ['ffill'] + error_indicators:\n",
    "        bool_series[col] = bool_series[col].astype(bool)\n",
    "    save_dataframe_in_HDF5(data_file_path, sensor_path, f\"bools\", bool_series)\n",
    "    del bool_series\n",
    "    \n",
    "    print('    Done')\n",
    "    print('')\n",
    "\n",
    "# clean up memory\n",
    "del full_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sewer_data\n",
      "├── G71F04R_Level1\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G71F04R_Level2\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G71F05R_LevelBasin\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G71F05R_LevelInlet\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G71F05R_position\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G71F06R_LevelInlet\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G71F68Y_LevelPS\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G71F68Yp1\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G71F68Yp1_power\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G71F68Yp2_power\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G72F040\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G73F010\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G80F11B_Level1\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G80F11B_Level2\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G80F13P_LevelPS\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G80F13Pp1_power\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G80F13Pp2_power\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "├── G80F66Y_Level1\n",
      "│   ├── bools\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   ├── clean\n",
      "│   │   ├── columns\n",
      "│   │   ├── data\n",
      "│   │   └── timestamps\n",
      "│   └── raw\n",
      "│       ├── columns\n",
      "│       ├── data\n",
      "│       └── timestamps\n",
      "└── G80F66Y_Level2\n",
      "    ├── bools\n",
      "    │   ├── columns\n",
      "    │   ├── data\n",
      "    │   └── timestamps\n",
      "    ├── clean\n",
      "    │   ├── columns\n",
      "    │   ├── data\n",
      "    │   └── timestamps\n",
      "    └── raw\n",
      "        ├── columns\n",
      "        ├── data\n",
      "        └── timestamps\n"
     ]
    }
   ],
   "source": [
    "#f = print_tree(data_file_path, save_path=FIGURES_DIR / 'single_series_dir')\n",
    "f = print_tree(data_file_path, group=sensor_group_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new metadata that is for each sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor name: G71F04R_Level1\n",
      "    StartTime: 2010-08-01T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G71F04R_Level2\n",
      "    StartTime: 2010-08-01T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G71F05R_LevelBasin\n",
      "    StartTime: 2010-08-01T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G71F05R_LevelInlet\n",
      "    StartTime: 2010-08-01T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G71F05R_position\n",
      "    StartTime: 2010-08-01T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G71F06R_LevelInlet\n",
      "    StartTime: 2010-08-01T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G71F68Y_LevelPS\n",
      "    StartTime: 2010-08-01T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G71F68Yp1\n",
      "    StartTime: 2010-08-01T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G71F68Yp1_power\n",
      "    StartTime: 2017-03-07T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G71F68Yp2_power\n",
      "    StartTime: 2017-03-07T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G72F040\n",
      "    StartTime: 2020-01-08T00:00:00.000000000\n",
      "    EndTime: 2020-03-23T00:00:00.000000000\n",
      "Sensor name: G73F010\n",
      "    StartTime: 2019-06-27T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G80F11B_Level1\n",
      "    StartTime: 2019-10-23T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G80F11B_Level2\n",
      "    StartTime: 2019-10-23T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G80F13P_LevelPS\n",
      "    StartTime: 2010-01-01T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G80F13Pp1_power\n",
      "    StartTime: 2010-01-01T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G80F13Pp2_power\n",
      "    StartTime: 2010-01-01T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G80F66Y_Level1\n",
      "    StartTime: 2019-10-23T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n",
      "Sensor name: G80F66Y_Level2\n",
      "    StartTime: 2019-10-23T00:00:00.000000000\n",
      "    EndTime: 2021-08-19T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "columns = ['IdMeasurement', 'Type', 'Navn', 'Unit', 'UnitAlias', 'obvious_min', 'obvious_max']\n",
    "metadata = external_metadata[columns].copy()\n",
    "# remove duplicates\n",
    "metadata.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "assert len(f.keys()) == metadata.shape[0], f\"metadata sensors: {metadata.shape[0]}, HDF5 sensors: {len(f.keys())}\"\n",
    "\n",
    "# fix the StartTime and EndTime\n",
    "for sensor_name, sensor_group in external_metadata.groupby('IdMeasurement'):\n",
    "    mask = metadata['IdMeasurement'] == sensor_name\n",
    "    metadata.loc[mask, 'StartTime'] = sensor_group['StartTime'].min()\n",
    "    metadata.loc[mask, 'EndTime'] = sensor_group['EndTime'].max()\n",
    "    print(f\"Sensor name: {sensor_name}\")\n",
    "    print(f\"    StartTime: {metadata.loc[mask, 'StartTime'].values[0]}\")\n",
    "    print(f\"    EndTime: {metadata.loc[mask, 'EndTime'].values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata is saved\n"
     ]
    }
   ],
   "source": [
    "# save the metadata\n",
    "# Save the metadata as a csv file\n",
    "metadata.to_csv(REFERENCE_DIR / 'sensor_metadata.csv', index=False)\n",
    "print(\"Metadata is saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IdMeasurement</th>\n",
       "      <th>Type</th>\n",
       "      <th>Navn</th>\n",
       "      <th>Unit</th>\n",
       "      <th>UnitAlias</th>\n",
       "      <th>obvious_min</th>\n",
       "      <th>obvious_max</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G71F04R_Level1</td>\n",
       "      <td>Level</td>\n",
       "      <td>Niv. Indløb 1</td>\n",
       "      <td>m</td>\n",
       "      <td>Meter</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>2021-08-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G71F04R_Level2</td>\n",
       "      <td>Level</td>\n",
       "      <td>Niv. Indløb 2</td>\n",
       "      <td>m</td>\n",
       "      <td>Meter</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>2021-08-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G71F05R_LevelBasin</td>\n",
       "      <td>Level</td>\n",
       "      <td>Niv. Skyllevandsbeh.</td>\n",
       "      <td>m</td>\n",
       "      <td>Meter</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>2021-08-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G71F05R_LevelInlet</td>\n",
       "      <td>Level</td>\n",
       "      <td>Niv. Indløb</td>\n",
       "      <td>m</td>\n",
       "      <td>Meter</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>2021-08-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G71F05R_position</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position throttle</td>\n",
       "      <td>m</td>\n",
       "      <td>Meter</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>2021-08-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        IdMeasurement      Type                  Navn Unit UnitAlias  \\\n",
       "0      G71F04R_Level1     Level         Niv. Indløb 1    m     Meter   \n",
       "1      G71F04R_Level2     Level         Niv. Indløb 2    m     Meter   \n",
       "2  G71F05R_LevelBasin     Level  Niv. Skyllevandsbeh.    m     Meter   \n",
       "3  G71F05R_LevelInlet     Level           Niv. Indløb    m     Meter   \n",
       "4    G71F05R_position  Position     Position throttle    m     Meter   \n",
       "\n",
       "   obvious_min  obvious_max   StartTime     EndTime  \n",
       "0        0.001          1.5  2010-08-01  2021-08-19  \n",
       "1        0.001          1.5  2010-08-01  2021-08-19  \n",
       "2        0.000          5.0  2010-08-01  2021-08-19  \n",
       "3        0.001          1.5  2010-08-01  2021-08-19  \n",
       "4        0.000          3.0  2010-08-01  2021-08-19  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the metadata\n",
    "metadata = pd.read_csv(REFERENCE_DIR / 'sensor_metadata.csv')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar for the Rain Gauge Data\n",
    "\n",
    "- Assume the rain data is of high quality\n",
    "- Only stores data when it is more than 0\n",
    "- Thus all data is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths\n",
    "#rain_gauges_path = EXTERNAL_DATA_DIR / 'Bellinge' / 'rain-gauge-data' / '#3a_Raingauges'\n",
    "rain_gauges_path = INTERIM_DATA_DIR / 'Bellinge' / 'rain-gauge-data'\n",
    "\n",
    "# contains = \"_ts\"\n",
    "# # list all the files in the folder\n",
    "files = os.listdir(rain_gauges_path)\n",
    "\n",
    "from fault_management_uds.config import rain_gauges\n",
    "\n",
    "assert len(files) == len(rain_gauges), f\"files: {len(files)}, rain_gauges: {len(rain_gauges)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: 5425.pkl\n",
      "    Done\n",
      "Loading data from file: 5427.pkl\n",
      "    Done\n"
     ]
    }
   ],
   "source": [
    "# create a group if it does not exist\n",
    "rain_gauge_group_path = single_series_group + '/rain_gauge_data'\n",
    "create_group(data_file_path, rain_gauge_group_path, verbose=False)\n",
    "\n",
    "for name in rain_gauges:\n",
    "    filename = f\"{name}.pkl\"\n",
    "    print(f\"Loading data from file: {filename}\")\n",
    "    data = pd.read_pickle(rain_gauges_path / filename)\n",
    "    # Save to the HDF5 file\n",
    "    save_dataframe_in_HDF5(data_file_path, rain_gauge_group_path, name, data)\n",
    "    print('    Done')\n",
    "\n",
    "# clean up memory\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rain_gauge_data\n",
      "├── 5425\n",
      "│   ├── columns\n",
      "│   ├── data\n",
      "│   └── timestamps\n",
      "└── 5427\n",
      "    ├── columns\n",
      "    ├── data\n",
      "    └── timestamps\n"
     ]
    }
   ],
   "source": [
    "f = print_tree(data_file_path, group=rain_gauge_group_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Creating the combined data\n",
    "\n",
    "\n",
    "We'll create three datasets\n",
    "- raw\n",
    "- clean\n",
    "- indicator\n",
    "\n",
    "where indicator will contain the following values\n",
    "- -1 is normal\n",
    "- 0 is missing\n",
    "- 1+ is error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_group = \"combined_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min time: 2010-01-01 00:00:00, Max time: 2021-08-19 00:00:00\n"
     ]
    }
   ],
   "source": [
    "min_time = external_metadata['StartTime'].min()\n",
    "max_time = external_metadata['EndTime'].max()\n",
    "\n",
    "print(f\"Min time: {min_time}, Max time: {max_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fault_management_uds.config import single_series_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First create empty datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a template df for the combined data\n",
    "time_range = pd.date_range(start=min_time, end=max_time, freq='1min')\n",
    "# default to zeros\n",
    "empty_data = np.zeros((len(time_range), len(single_series_order)))\n",
    "empty_df = pd.DataFrame(empty_data, columns=single_series_order)\n",
    "empty_df['time'] = time_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the combined data to be created\n",
    "combined_series = ['raw', 'clean', 'indicator']\n",
    "\n",
    "series_name = 'raw'\n",
    "# save the data, 0 by default\n",
    "save_dataframe_in_HDF5(data_file_path, combined_data_group, series_name, empty_df)\n",
    "\n",
    "series_name = 'clean'\n",
    "# save the data, 0 by default\n",
    "save_dataframe_in_HDF5(data_file_path, combined_data_group, series_name, empty_df)\n",
    "\n",
    "# series_name = 'indicator'\n",
    "# # save the data, 0 by default\n",
    "# # indicators will be 0 for no data, 1 for normal, 2 for error\n",
    "# save_dataframe_in_HDF5(data_file_path, combined_data_group, series_name, empty_df)\n",
    "\n",
    "# series_name = 'full_indicator'\n",
    "# # save the data, 0 by default\n",
    "# # indicators will be 0 for no data, 1 for normal, 2 for error\n",
    "# save_dataframe_in_HDF5(data_file_path, combined_data_group, series_name, empty_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_data\n",
      "├── clean\n",
      "│   ├── columns\n",
      "│   ├── data\n",
      "│   └── timestamps\n",
      "└── raw\n",
      "    ├── columns\n",
      "    ├── data\n",
      "    └── timestamps\n"
     ]
    }
   ],
   "source": [
    "#f = print_tree(data_file_path, save_path=FIGURES_DIR / 'single_series_dir')\n",
    "f = print_tree(data_file_path, group=\"combined_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the data to combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'single_series/rain_gauge_data'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain_gauge_group_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data: raw\n",
      "    Rain gauge: 5425\n",
      "    Rain gauge: 5427\n",
      "        Data saved in group '/combined_data/raw'\n",
      "Combined data: clean\n",
      "    Rain gauge: 5425\n",
      "    Rain gauge: 5427\n",
      "        Data saved in group '/combined_data/clean'\n"
     ]
    }
   ],
   "source": [
    "# iterate the combined data\n",
    "for combined in ['raw', 'clean']:\n",
    "    print(f\"Combined data: {combined}\")\n",
    "    combined_data, start_idx, end_idx, column_indices = load_dataframe_from_HDF5(data_file_path, f\"{combined_data_group}/{combined}\")\n",
    "    # iterate the rain gauges\n",
    "    for rain_gauge in rain_gauges:\n",
    "        print(f\"    Rain gauge: {rain_gauge}\")\n",
    "        # load all the data\n",
    "        rain_gauge_path = f\"{rain_gauge_group_path}/{rain_gauge}\"\n",
    "        df, _, _, _ = load_dataframe_from_HDF5(data_file_path, rain_gauge_path)\n",
    "        # rename the columns\n",
    "        df = df.rename(columns={'value': rain_gauge})\n",
    "\n",
    "        # Insert the rain gauge data from df into combined_data without changing combined_data's index\n",
    "        combined_data[rain_gauge] = df[rain_gauge].reindex(combined_data.index)\n",
    "\n",
    "    # save the combined data\n",
    "    update_filtered_data_in_HDF5(data_file_path, f\"{combined_data_group}/{combined}\", combined_data, start_idx, end_idx, column_indices)\n",
    "\n",
    "# NOTE: no need to iterate the indicator data as it the rain gauges have no errors or missing data\n",
    "\n",
    "# clean up memory\n",
    "del combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-sewer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data: raw\n",
      "    Sensor: G80F11B_Level1\n",
      "    Sensor: G80F11B_Level2\n",
      "    Sensor: G80F66Y_Level1\n",
      "    Sensor: G80F66Y_Level2\n",
      "    Sensor: G80F13P_LevelPS\n",
      "    Sensor: G80F13Pp1_power\n",
      "    Sensor: G80F13Pp2_power\n",
      "    Sensor: G73F010\n",
      "    Sensor: G72F040\n",
      "    Sensor: G71F05R_LevelInlet\n",
      "    Sensor: G71F05R_LevelBasin\n",
      "    Sensor: G71F05R_position\n",
      "    Sensor: G71F04R_Level1\n",
      "    Sensor: G71F04R_Level2\n",
      "    Sensor: G71F06R_LevelInlet\n",
      "    Sensor: G71F68Y_LevelPS\n",
      "    Sensor: G71F68Yp1\n",
      "    Sensor: G71F68Yp1_power\n",
      "    Sensor: G71F68Yp2_power\n",
      "        Data saved in group '/combined_data/raw'\n",
      "Combined data: clean\n",
      "    Sensor: G80F11B_Level1\n",
      "    Sensor: G80F11B_Level2\n",
      "    Sensor: G80F66Y_Level1\n",
      "    Sensor: G80F66Y_Level2\n",
      "    Sensor: G80F13P_LevelPS\n",
      "    Sensor: G80F13Pp1_power\n",
      "    Sensor: G80F13Pp2_power\n",
      "    Sensor: G73F010\n",
      "    Sensor: G72F040\n",
      "    Sensor: G71F05R_LevelInlet\n",
      "    Sensor: G71F05R_LevelBasin\n",
      "    Sensor: G71F05R_position\n",
      "    Sensor: G71F04R_Level1\n",
      "    Sensor: G71F04R_Level2\n",
      "    Sensor: G71F06R_LevelInlet\n",
      "    Sensor: G71F68Y_LevelPS\n",
      "    Sensor: G71F68Yp1\n",
      "    Sensor: G71F68Yp1_power\n",
      "    Sensor: G71F68Yp2_power\n",
      "        Data saved in group '/combined_data/clean'\n"
     ]
    }
   ],
   "source": [
    "# next step is to insert the in-sewer data into the combined data\n",
    "# iterate the combined data\n",
    "for combined in ['raw', 'clean']:\n",
    "    print(f\"Combined data: {combined}\")\n",
    "    combined_data, start_idx, end_idx, column_indices = load_dataframe_from_HDF5(data_file_path, f\"{combined_data_group}/{combined}\")\n",
    "    # iterate the sensors\n",
    "    for sensor in natural_sensor_order:\n",
    "        print(f\"    Sensor: {sensor}\")\n",
    "        # load all the data, NOTE: use the combined suffix\n",
    "        sensor_path = f\"{sensor_group_path}/{sensor}/{combined}\"\n",
    "        df, _, _, _ = load_dataframe_from_HDF5(data_file_path, sensor_path)\n",
    "        # rename the columns\n",
    "        df = df.rename(columns={'value': sensor})\n",
    "\n",
    "        # Insert the sensor data from df into combined_data without changing combined_data's index\n",
    "        combined_data[sensor] = df[sensor].reindex(combined_data.index)\n",
    "\n",
    "    # save the combined data\n",
    "    update_filtered_data_in_HDF5(data_file_path, f\"{combined_data_group}/{combined}\", combined_data, start_idx, end_idx, column_indices)\n",
    "\n",
    "\n",
    "# clean up memory\n",
    "del combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Stop right there!",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Stop right there!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arond.jacobsen/anaconda3/envs/thesis/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# stop system from running\n",
    "raise SystemExit(\"Stop right there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicator data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data: indicator\n",
      "    Sensor: G80F11B_Level1\n",
      "    Sensor: G80F11B_Level2\n",
      "    Sensor: G80F66Y_Level1\n",
      "    Sensor: G80F66Y_Level2\n",
      "    Sensor: G80F13P_LevelPS\n",
      "    Sensor: G80F13Pp1_power\n",
      "    Sensor: G80F13Pp2_power\n",
      "    Sensor: G73F010\n",
      "    Sensor: G72F040\n",
      "    Sensor: G71F05R_LevelInlet\n",
      "    Sensor: G71F05R_LevelBasin\n",
      "    Sensor: G71F05R_position\n",
      "    Sensor: G71F04R_Level1\n",
      "    Sensor: G71F04R_Level2\n",
      "    Sensor: G71F06R_LevelInlet\n",
      "    Sensor: G71F68Y_LevelPS\n",
      "    Sensor: G71F68Yp1\n",
      "    Sensor: G71F68Yp1_power\n",
      "    Sensor: G71F68Yp2_power\n",
      "    Rain gauge: 5425\n",
      "    Rain gauge: 5427\n",
      "        Data saved in group '/combined_data/indicator'\n"
     ]
    }
   ],
   "source": [
    "# next step is to insert the in-sewer data into the combined data\n",
    "# iterate the combined data\n",
    "combined = 'indicator'\n",
    "print(f\"Combined data: {combined}\")\n",
    "combined_data, start_idx, end_idx, column_indices = load_dataframe_from_HDF5(data_file_path, f\"{combined_data_group}/{combined}\")\n",
    "\n",
    "\n",
    "# iterate the sensors\n",
    "for sensor in natural_sensor_order:\n",
    "    print(f\"    Sensor: {sensor}\")\n",
    "    # Indicator with 1 for data and 0 for no data using the raw data\n",
    "    sensor_path = f\"{sensor_group_path}/{sensor}/{sensor}_clean\"\n",
    "    df, _, _, _ = load_dataframe_from_HDF5(data_file_path, sensor_path)\n",
    "\n",
    "    # TODO: create indicator based on the function in processed!\n",
    "    # TODO: probably save it somewhere else than in load data? maybe in processed data?\n",
    "\n",
    "\n",
    "    # Indicator with 1 for data and 0 for no data\n",
    "    df['indicator'] = df['value'].notna().astype(int)\n",
    "    # Set 0 valued data to -1 indicator\n",
    "    df.loc[df['value'] == 0, 'indicator'] = -1\n",
    "\n",
    "    # Insert the sensor data from df into combined_data without changing combined_data's index\n",
    "    #combined_data[sensor] = df['indicator'].reindex(combined_data.index)\n",
    "    combined_data[sensor] = df['indicator'].reindex(combined_data.index).combine_first(combined_data[sensor])\n",
    "\n",
    "    # Indicator with 2 for error otherwise remove rows\n",
    "    sensor_path = f\"{sensor_group_path}/{sensor}/{sensor}_bools\"\n",
    "    df, _, _, _ = load_dataframe_from_HDF5(data_file_path, sensor_path, columns=error_indicators)\n",
    "    # Indicator with 2 for error otherwise remove rows\n",
    "    df['indicator'] = df.any(axis=1).astype(int) * 2\n",
    "    # drop rows with no errors\n",
    "    df = df[df['indicator'] > 0]\n",
    "\n",
    "    # Insert the sensor data from df into combined_data without changing combined_data's index\n",
    "    #combined_data[sensor] = df['indicator'].reindex(combined_data.index)\n",
    "    combined_data[sensor] = df['indicator'].reindex(combined_data.index).combine_first(combined_data[sensor])\n",
    "\n",
    "# similar for the rain gauges\n",
    "for rain_gauge in rain_gauges:\n",
    "    print(f\"    Rain gauge: {rain_gauge}\")\n",
    "    # Indicator with 1 for data and 0 for no data using the raw data\n",
    "    rain_gauge_path = f\"{rain_gauge_group_path}/{rain_gauge}\"\n",
    "    df, _, _, _ = load_dataframe_from_HDF5(data_file_path, rain_gauge_path)\n",
    "\n",
    "    # Indicator with 1 for data and 0 for no data\n",
    "    df['indicator'] = df['value'].notna().astype(int)\n",
    "    # Set 0 valued data to -1 indicator\n",
    "    df.loc[df['value'] == 0, 'indicator'] = -1\n",
    "\n",
    "    # there are no errors\n",
    "\n",
    "    # Insert the rain gauge data from df into combined_data without changing combined_data's index\n",
    "    #combined_data[rain_gauge] = df['indicator'].reindex(combined_data.index)\n",
    "    combined_data[rain_gauge] = df['indicator'].reindex(combined_data.index).combine_first(combined_data[rain_gauge])\n",
    "\n",
    "\n",
    "# save the combined data\n",
    "update_filtered_data_in_HDF5(data_file_path, f\"{combined_data_group}/{combined}\", combined_data, start_idx, end_idx, column_indices)\n",
    "    \n",
    "# clean up memory\n",
    "del combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data: indicator\n",
      "Sensor: G80F11B_Level1\n",
      "Sensor: G80F11B_Level2\n",
      "Sensor: G80F66Y_Level1\n",
      "Sensor: G80F66Y_Level2\n",
      "Sensor: G80F13P_LevelPS\n",
      "Sensor: G80F13Pp1_power\n",
      "Sensor: G80F13Pp2_power\n",
      "Sensor: G73F010\n",
      "Sensor: G72F040\n",
      "Sensor: G71F05R_LevelInlet\n",
      "Sensor: G71F05R_LevelBasin\n",
      "Sensor: G71F05R_position\n",
      "Sensor: G71F04R_Level1\n",
      "Sensor: G71F04R_Level2\n",
      "Sensor: G71F06R_LevelInlet\n",
      "Sensor: G71F68Y_LevelPS\n",
      "Sensor: G71F68Yp1\n",
      "Sensor: G71F68Yp1_power\n",
      "Sensor: G71F68Yp2_power\n"
     ]
    }
   ],
   "source": [
    "combined = 'indicator'\n",
    "print(f\"Combined data: {combined}\")\n",
    "combined_data, start_idx, end_idx, column_indices = load_dataframe_from_HDF5(data_file_path, f\"{combined_data_group}/{combined}\")\n",
    "\n",
    "\n",
    "# save to each sensor\n",
    "for sensor_name in natural_sensor_order:\n",
    "    print(f\"Sensor: {sensor_name}\")\n",
    "    sensor_indicator = combined_data[sensor_name].copy().to_frame()\n",
    "    sensor_indicator.reset_index(inplace=True, drop=False, names='time')\n",
    "    sensor_indicator.columns = ['time', 'value']\n",
    "    sensor_path = f\"{sensor_group_path}/{sensor_name}\"\n",
    "    save_dataframe_in_HDF5(data_file_path, sensor_path, f\"{sensor_name}_{combined}\", sensor_indicator)\n",
    "\n",
    "# clean up memory\n",
    "del combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to create a indicator columns, but what if some errors occur together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we have 5 errros, and how often can we combine them in sets of 1, 2, 3, 4, 5\n",
    "(5*4/2) * 5\n",
    "# i.e. 50 different indicators if we run multilabel error indicators...\n",
    "# instead do a smart check to see what errors occur together and create a new indicator for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor: G80F66Y_Level1, max sum: 2, total higher than 1: 485\n",
      "Sensor: G80F66Y_Level2, max sum: 2, total higher than 1: 481\n",
      "Sensor: G80F13P_LevelPS, max sum: 2, total higher than 1: 1759\n",
      "Sensor: G71F05R_LevelInlet, max sum: 3, total higher than 1: 48288\n",
      "Sensor: G71F05R_LevelBasin, max sum: 2, total higher than 1: 81701\n",
      "Sensor: G71F04R_Level1, max sum: 2, total higher than 1: 7217059\n",
      "Sensor: G71F04R_Level2, max sum: 2, total higher than 1: 455816\n",
      "Sensor: G71F06R_LevelInlet, max sum: 2, total higher than 1: 46626\n",
      "Sensor: G71F68Y_LevelPS, max sum: 2, total higher than 1: 13751\n"
     ]
    }
   ],
   "source": [
    "f = print_tree(data_file_path, print_tree=False)\n",
    "\n",
    "# collect all the errors for all the sensors\n",
    "more_one_error = np.array([[False]*len(error_indicators)])\n",
    "\n",
    "for sensor in natural_sensor_order:\n",
    "    sensor_bools_path = sensor_group_path + '/' + sensor + '/' + sensor + '_bools'\n",
    "    data, _, _, _ = load_dataframe_from_HDF5(data_file_path, sensor_bools_path, columns=error_indicators)\n",
    "    data = data.astype(bool)\n",
    "    if data.sum(axis=1).max() > 1:\n",
    "        print(f\"Sensor: {sensor}, max sum: {data.sum(axis=1).max()}, total higher than 1: {data.sum(axis=1).sum()}\")\n",
    "        # combine the data, data[data.sum(axis=1) > 1]\n",
    "        more_one_error = np.concatenate([more_one_error, data[data.sum(axis=1) > 1]])\n",
    "\n",
    "# remove the first row\n",
    "more_one_error = more_one_error[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many combinations we have\n",
    "error_combinations = {}\n",
    "for row in more_one_error:\n",
    "    # get the index of there the error is True\n",
    "    errors = list(set(np.where(row)[0]))\n",
    "    error_name_list = [error_indicators[int(error)] for error in errors]\n",
    "\n",
    "    # check if list is in the dictionary\n",
    "    if str(error_name_list) in error_combinations:\n",
    "        error_combinations[str(error_name_list)] += 1\n",
    "    else:\n",
    "        error_combinations[str(error_name_list)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"['stamp', 'outbound']\": 2098,\n",
       " \"['outbound', 'frozen']\": 4281,\n",
       " \"['stamp', 'outbound', 'frozen']\": 1,\n",
       " \"['man_remove', 'outbound']\": 3249413}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations: 4\n",
      "\n",
      "Errors: ['stamp', 'outbound'], count: 2098\n",
      "Errors: ['outbound', 'frozen'], count: 4281\n",
      "Errors: ['stamp', 'outbound', 'frozen'], count: 1\n",
      "Errors: ['man_remove', 'outbound'], count: 3249413\n"
     ]
    }
   ],
   "source": [
    "# iterate the keys\n",
    "print(f\"Total combinations: {len(error_combinations)}\\n\")\n",
    "for error_name_list in error_combinations.keys():\n",
    "    # # extract the errors\n",
    "    # errors = key[1:-1].split(',')\n",
    "    # errors = [error_indicators[int(error)] for error in errors]\n",
    "    print(f\"Errors: {error_name_list}, count: {error_combinations[error_name_list]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating an indicator for the combined that, we'll use these combinations as well as indicators for errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an indicator to data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a json file with indicator to error\n",
    "error_indicator_2_dq = {}\n",
    "error_list_2_indicator = {}\n",
    "\n",
    "# add the no error indicator\n",
    "error_indicator_2_dq[0] = 'No errors'\n",
    "error_list_2_indicator[tuple([False]*len(error_indicators))] = 0\n",
    "\n",
    "# iterate the errors\n",
    "for error in error_indicators:\n",
    "    highest_indicator = len(error_indicator_2_dq.keys())\n",
    "    error_indicator_2_dq[highest_indicator] = bools_2_meta[error]['alias']\n",
    "    #error.replace('_', ' ').capitalize().replace('Man', 'Manual').replace('remove', 'removal')\n",
    "    error_list = [True if error == error_indicator else False for error_indicator in error_indicators]\n",
    "    error_list_2_indicator[tuple(error_list)] = highest_indicator\n",
    "\n",
    "# save the json file\n",
    "with open(REFERENCE_DIR / 'error_indicator_2_dq.json', 'w') as f:\n",
    "    json.dump(error_indicator_2_dq, f)\n",
    "\n",
    "with open(REFERENCE_DIR / 'error_list_2_indicator.json', 'w') as f:\n",
    "    # cannot save keys as tuple, convert to str\n",
    "    error_list_2_indicator = {str(key): value for key, value in error_list_2_indicator.items()}\n",
    "    json.dump(error_list_2_indicator, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file\n",
    "with open(REFERENCE_DIR / 'error_indicator_2_dq.json', 'r') as f:\n",
    "    error_indicator_2_dq = json.load(f)\n",
    "    # convert the keys to integers\n",
    "    error_indicator_2_dq = {int(key): value for key, value in error_indicator_2_dq.items()}\n",
    "\n",
    "with open(REFERENCE_DIR / 'error_list_2_indicator.json', 'r') as f:\n",
    "    error_list_2_indicator = json.load(f)\n",
    "    # convert the keys to integers\n",
    "    error_list_2_indicator = {eval(key): value for key, value in error_list_2_indicator.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the combinations to the dictionary\n",
    "\n",
    "for error_name_list in error_combinations.keys():\n",
    "    # extract the errors\n",
    "    error_name_list = eval(error_name_list)\n",
    "    error_list = [True if error_name in error_name_list else False for error_name in error_indicators]\n",
    "    errors_alias_list = [bools_2_meta[error_name]['alias'] for error_name in error_name_list]\n",
    "    # create a error1, error2,... \n",
    "    errors_str = ', '.join(errors_alias_list)\n",
    "    # add the indicator\n",
    "    highest_indicator = len(error_indicator_2_dq.keys())\n",
    "    error_indicator_2_dq[highest_indicator] = errors_str\n",
    "    error_list_2_indicator[tuple(error_list)] = highest_indicator\n",
    "\n",
    "# save the json file\n",
    "with open(REFERENCE_DIR / 'error_indicator_2_dq.json', 'w') as f:\n",
    "    json.dump(error_indicator_2_dq, f)\n",
    "with open(REFERENCE_DIR / 'error_list_2_indicator.json', 'w') as f:\n",
    "    # cannot save keys as tuple, convert to str\n",
    "    error_list_2_indicator = {str(key): value for key, value in error_list_2_indicator.items()}\n",
    "    json.dump(error_list_2_indicator, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file\n",
    "with open(REFERENCE_DIR / 'error_indicator_2_dq.json', 'r') as f:\n",
    "    error_indicator_2_dq = json.load(f)\n",
    "    # convert the keys to integers\n",
    "    error_indicator_2_dq = {int(key): value for key, value in error_indicator_2_dq.items()}\n",
    "\n",
    "with open(REFERENCE_DIR / 'error_list_2_indicator.json', 'r') as f:\n",
    "    error_list_2_indicator = json.load(f)\n",
    "    # convert the keys to integers\n",
    "    error_list_2_indicator = {eval(key): value for key, value in error_list_2_indicator.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 315.38 seconds, i.e. 5.26 minutes\n",
      "Current time: 1729838200.324862\n"
     ]
    }
   ],
   "source": [
    "# total runtime\n",
    "runtime_end = time.time()\n",
    "runtime = runtime_end - runtime_start\n",
    "print(f\"Total runtime: {runtime:.2f} seconds, i.e. {runtime/60:.2f} minutes\")\n",
    "\n",
    "# current time\n",
    "print(f\"Current time: {runtime_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Stop right there!",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Stop right there!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arond.jacobsen/anaconda3/envs/thesis/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# stop system from running\n",
    "raise SystemExit(\"Stop right there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Not going to use the full indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a indicator series for each sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data: full_indicator\n",
      "    Loading data from group: combined_data/full_indicator\n",
      "Sensor: G80F11B_Level1\n",
      "    Loading data from group: single_series/sewer_data/G80F11B_Level1/G80F11B_Level1_raw\n",
      "    Loading data from group: single_series/sewer_data/G80F11B_Level1/G80F11B_Level1_errors\n",
      "Sensor: G80F11B_Level2\n",
      "    Loading data from group: single_series/sewer_data/G80F11B_Level2/G80F11B_Level2_raw\n",
      "    Loading data from group: single_series/sewer_data/G80F11B_Level2/G80F11B_Level2_errors\n",
      "Sensor: G80F66Y_Level1\n",
      "    Loading data from group: single_series/sewer_data/G80F66Y_Level1/G80F66Y_Level1_raw\n",
      "    Loading data from group: single_series/sewer_data/G80F66Y_Level1/G80F66Y_Level1_errors\n",
      "Sensor: G80F66Y_Level2\n",
      "    Loading data from group: single_series/sewer_data/G80F66Y_Level2/G80F66Y_Level2_raw\n",
      "    Loading data from group: single_series/sewer_data/G80F66Y_Level2/G80F66Y_Level2_errors\n",
      "Sensor: G80F13P_LevelPS\n",
      "    Loading data from group: single_series/sewer_data/G80F13P_LevelPS/G80F13P_LevelPS_raw\n",
      "    Loading data from group: single_series/sewer_data/G80F13P_LevelPS/G80F13P_LevelPS_errors\n",
      "Sensor: G80F13Pp1_power\n",
      "    Loading data from group: single_series/sewer_data/G80F13Pp1_power/G80F13Pp1_power_raw\n",
      "    Loading data from group: single_series/sewer_data/G80F13Pp1_power/G80F13Pp1_power_errors\n",
      "Sensor: G80F13Pp2_power\n",
      "    Loading data from group: single_series/sewer_data/G80F13Pp2_power/G80F13Pp2_power_raw\n",
      "    Loading data from group: single_series/sewer_data/G80F13Pp2_power/G80F13Pp2_power_errors\n",
      "Sensor: G73F010\n",
      "    Loading data from group: single_series/sewer_data/G73F010/G73F010_raw\n",
      "    Loading data from group: single_series/sewer_data/G73F010/G73F010_errors\n",
      "Sensor: G72F040\n",
      "    Loading data from group: single_series/sewer_data/G72F040/G72F040_raw\n",
      "    Loading data from group: single_series/sewer_data/G72F040/G72F040_errors\n",
      "Sensor: G71F05R_LevelInlet\n",
      "    Loading data from group: single_series/sewer_data/G71F05R_LevelInlet/G71F05R_LevelInlet_raw\n",
      "    Loading data from group: single_series/sewer_data/G71F05R_LevelInlet/G71F05R_LevelInlet_errors\n",
      "Sensor: G71F05R_LevelBasin\n",
      "    Loading data from group: single_series/sewer_data/G71F05R_LevelBasin/G71F05R_LevelBasin_raw\n",
      "    Loading data from group: single_series/sewer_data/G71F05R_LevelBasin/G71F05R_LevelBasin_errors\n",
      "Sensor: G71F05R_position\n",
      "    Loading data from group: single_series/sewer_data/G71F05R_position/G71F05R_position_raw\n",
      "    Loading data from group: single_series/sewer_data/G71F05R_position/G71F05R_position_errors\n",
      "Sensor: G71F04R_Level1\n",
      "    Loading data from group: single_series/sewer_data/G71F04R_Level1/G71F04R_Level1_raw\n",
      "    Loading data from group: single_series/sewer_data/G71F04R_Level1/G71F04R_Level1_errors\n",
      "Sensor: G71F04R_Level2\n",
      "    Loading data from group: single_series/sewer_data/G71F04R_Level2/G71F04R_Level2_raw\n",
      "    Loading data from group: single_series/sewer_data/G71F04R_Level2/G71F04R_Level2_errors\n",
      "Sensor: G71F06R_LevelInlet\n",
      "    Loading data from group: single_series/sewer_data/G71F06R_LevelInlet/G71F06R_LevelInlet_raw\n",
      "    Loading data from group: single_series/sewer_data/G71F06R_LevelInlet/G71F06R_LevelInlet_errors\n",
      "Sensor: G71F68Y_LevelPS\n",
      "    Loading data from group: single_series/sewer_data/G71F68Y_LevelPS/G71F68Y_LevelPS_raw\n",
      "    Loading data from group: single_series/sewer_data/G71F68Y_LevelPS/G71F68Y_LevelPS_errors\n",
      "Sensor: G71F68Yp1\n",
      "    Loading data from group: single_series/sewer_data/G71F68Yp1/G71F68Yp1_raw\n",
      "    Loading data from group: single_series/sewer_data/G71F68Yp1/G71F68Yp1_errors\n",
      "Sensor: G71F68Yp1_power\n",
      "    Loading data from group: single_series/sewer_data/G71F68Yp1_power/G71F68Yp1_power_raw\n",
      "    Loading data from group: single_series/sewer_data/G71F68Yp1_power/G71F68Yp1_power_errors\n",
      "Sensor: G71F68Yp2_power\n",
      "    Loading data from group: single_series/sewer_data/G71F68Yp2_power/G71F68Yp2_power_raw\n",
      "    Loading data from group: single_series/sewer_data/G71F68Yp2_power/G71F68Yp2_power_errors\n",
      "        Data saved in group '/combined_data/full_indicator'\n"
     ]
    }
   ],
   "source": [
    "# next step is to insert the in-sewer data into the combined data\n",
    "# iterate the combined data\n",
    "combined = 'full_indicator'\n",
    "print(f\"Combined data: {combined}\")\n",
    "combined_data, start_idx, end_idx, column_indices = load_dataframe_from_HDF5(data_file_path, f\"{combined_data_group}/{combined}\")\n",
    "\n",
    "# iterate the sensors\n",
    "for sensor in natural_sensor_order:\n",
    "    print(f\"Sensor: {sensor}\")\n",
    "    # Indicator with 1 for data and 0 for no data using the raw data\n",
    "    sensor_path = f\"{sensor_group_path}/{sensor}/{sensor}_raw\"\n",
    "    df, _, _, _ = load_dataframe_from_HDF5(data_file_path, sensor_path)\n",
    "\n",
    "    # Indicator with 1 for data and 0 for no data\n",
    "    df['indicator'] = df['value'].notna().astype(int)\n",
    "\n",
    "    # Insert the sensor data from df into combined_data without changing combined_data's index\n",
    "    combined_data[sensor] = df['indicator'].reindex(combined_data.index).combine_first(combined_data[sensor])\n",
    "\n",
    "    # Indicator with 2 for error otherwise remove rows\n",
    "    sensor_path = f\"{sensor_group_path}/{sensor}/{sensor}_errors\"\n",
    "    df, _, _, _ = load_dataframe_from_HDF5(data_file_path, sensor_path)\n",
    "\n",
    "    # First filter out the rows with no errors\n",
    "    df['any_error'] = df.any(axis=1).astype(int)\n",
    "    # drop rows with no errors\n",
    "    df = df[df['any_error'] > 0]\n",
    "    df = df[error_indicators]\n",
    "    df['error_list'] = df.apply(tuple, axis=1)\n",
    "    df['indicator'] = df['error_list'].map(error_list_2_indicator)\n",
    "\n",
    "    # Insert the sensor data from df into combined_data without changing combined_data's index\n",
    "    combined_data[sensor] = df['indicator'].reindex(combined_data.index).combine_first(combined_data[sensor])\n",
    "\n",
    "# save the combined data\n",
    "update_filtered_data_in_HDF5(data_file_path, f\"{combined_data_group}/{combined}\", combined_data, start_idx, end_idx, column_indices)\n",
    "\n",
    "\n",
    "# clean up memory\n",
    "del combined_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data: full_indicator\n",
      "    Loading data from group: combined_data/full_indicator\n",
      "Sensor: G80F11B_Level1\n",
      "Sensor: G80F11B_Level2\n",
      "Sensor: G80F66Y_Level1\n",
      "Sensor: G80F66Y_Level2\n",
      "Sensor: G80F13P_LevelPS\n",
      "Sensor: G80F13Pp1_power\n",
      "Sensor: G80F13Pp2_power\n",
      "Sensor: G73F010\n",
      "Sensor: G72F040\n",
      "Sensor: G71F05R_LevelInlet\n",
      "Sensor: G71F05R_LevelBasin\n",
      "Sensor: G71F05R_position\n",
      "Sensor: G71F04R_Level1\n",
      "Sensor: G71F04R_Level2\n",
      "Sensor: G71F06R_LevelInlet\n",
      "Sensor: G71F68Y_LevelPS\n",
      "Sensor: G71F68Yp1\n",
      "Sensor: G71F68Yp1_power\n",
      "Sensor: G71F68Yp2_power\n"
     ]
    }
   ],
   "source": [
    "combined = 'full_indicator'\n",
    "print(f\"Combined data: {combined}\")\n",
    "combined_data, start_idx, end_idx, column_indices = load_dataframe_from_HDF5(data_file_path, f\"{combined_data_group}/{combined}\")\n",
    "\n",
    "\n",
    "# save to each sensor\n",
    "for sensor_name in natural_sensor_order:\n",
    "    print(f\"Sensor: {sensor_name}\")\n",
    "    sensor_indicator = combined_data[sensor_name].copy().to_frame()\n",
    "    sensor_indicator.reset_index(inplace=True, drop=False, names='time')\n",
    "    sensor_indicator.columns = ['time', 'value']\n",
    "    sensor_path = f\"{sensor_group_path}/{sensor_name}\"\n",
    "    save_dataframe_in_HDF5(data_file_path, sensor_path, f\"{sensor_name}_{combined}\", sensor_indicator)\n",
    "\n",
    "# clean up memory\n",
    "del combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How often is clean nan but not raw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = print_tree(data_file_path, print_tree=False)\n",
    "\n",
    "raw_clean_mismatch = {}\n",
    "\n",
    "for sensor in natural_sensor_order:\n",
    "    sensor_raw_path = sensor_group_path + '/' + sensor + '/' + sensor + '_raw' + '/data'\n",
    "    raw = f[sensor_raw_path].nxdata\n",
    "    sensor_clean_path = sensor_group_path + '/' + sensor + '/' + sensor + '_clean' + '/data'\n",
    "    clean = f[sensor_clean_path].nxdata\n",
    "    # count how often clean is nan but raw is not (numpy arrays)\n",
    "    mismatch = np.sum(np.isnan(clean) & ~np.isnan(raw))\n",
    "    raw_clean_mismatch[sensor] = mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how often the interpolation failed to fill the data gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
