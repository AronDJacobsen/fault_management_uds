{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Series modelling\n",
    "\n",
    "*Masked Multi-Step Autoregressive Regression*\n",
    "\n",
    "\n",
    "#### Contents\n",
    "\n",
    "1. [Dataset](#1)\n",
    "2. [Model](#2)\n",
    "3. [Training](#3)\n",
    "4. [Evaluation](#4)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial design choices:\n",
    "- *Autoregressive*: predict the next value based on the previous values.\n",
    "- *Rain data*: include the complete rain series.\n",
    "- *Masking*: mask to predict the the last 1 minute of the sensor data.\n",
    "- *Normalize*: normalize the data to a range of 0.1-1, and mask missing values with 0.\n",
    "- *Loss function*: mask missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible experiments\n",
    "\n",
    "- Predict residuals/changes\n",
    "- Multi-step-ahead scheduling\n",
    "- Quantiles?\n",
    "- Data augmentation\n",
    "- Utilize Mike predictions for training or evaluation\n",
    "- Alternate masking for multi-sensor and comparison?\n",
    "- 1-minute masking or 5-minute masking, or alternate?\n",
    "- include both rainfall sensors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### TODO:\n",
    "\n",
    "- figures wrt masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "\n",
    "from fault_management_uds.data.HDF5_functions import print_tree, load_dataframe_from_HDF5\n",
    "from fault_management_uds.data.process import remove_nans_from_start_end\n",
    "from fault_management_uds.config import indicator_2_meta, bools_2_meta, error_indicators, natural_sensor_order\n",
    "from fault_management_uds.data.load import import_external_metadata, import_metadata\n",
    "\n",
    "\n",
    "from fault_management_uds.config import PROJ_ROOT\n",
    "from fault_management_uds.config import DATA_DIR, RAW_DATA_DIR, INTERIM_DATA_DIR, PROCESSED_DATA_DIR, EXTERNAL_DATA_DIR\n",
    "from fault_management_uds.config import MODELS_DIR, REPORTS_DIR, FIGURES_DIR, REFERENCE_DIR\n",
    "\n",
    "# set random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = PROCESSED_DATA_DIR / 'Bellinge.h5'\n",
    "external_metadata = import_metadata(REFERENCE_DIR / 'external_metadata.csv')\n",
    "metadata = import_metadata(REFERENCE_DIR / 'sensor_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "- Create a dataset class for the single series\n",
    "- Split the dataset into train and test sets\n",
    "- Normalize the dataset\n",
    "- Create a dataloader for the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length=50):\n",
    "\n",
    "\n",
    "        if len(data) <= sequence_length:\n",
    "            raise ValueError(\"Dataset size must be larger than sequence_length.\")\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: should it be e.g. data.values()? what is first dim?\n",
    "        x = self.data[idx:idx + self.sequence_length]\n",
    "        y = self.data[idx + self.sequence_length]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(data, train_index, val_index, test_index, sequence_length=50):\n",
    "    # normalizer\n",
    "    scaler = MinMaxScaler() # TODO: does it normalize column wise?\n",
    "    train_set = scaler.fit_transform(data[train_index])\n",
    "    train_data = scaler.transform(train_set)\n",
    "    val_data = scaler.transform(data[val_index])\n",
    "    test_data = scaler.transform(data[test_index])\n",
    "\n",
    "    # create the datasets\n",
    "    train_dataset = SensorDataset(train_data, sequence_length)\n",
    "    val_dataset = SensorDataset(val_data, sequence_length)\n",
    "    test_dataset = SensorDataset(test_data, sequence_length)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "- Create a `LSTM` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM Layer with dropout\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Weight initialization\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate through LSTM\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "\n",
    "        # Fully connected layer on the last hidden state\n",
    "        out = self.fc(out[:, -1, :])  # Use the last time step's output\n",
    "        return out\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize LSTM weights and biases\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "        # Initialize FC layer\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorLSTM(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, learning_rate=0.001, dropout=0.2):\n",
    "        super(SensorLSTM, self).__init__()\n",
    "        self.save_hyperparameters()  # Save hyperparameters for easier model checkpointing\n",
    "        self.model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "        self.criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        # Compute additional metrics (e.g., MAE)\n",
    "        mae = nn.L1Loss()(y_hat, y)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_mae', mae, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "- Train the model on the dataset\n",
    "- Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_splits(data, dataset_args):\n",
    "    splits = []\n",
    "    # split by percentage\n",
    "    if dataset_args['n_splits'] == 1:\n",
    "        # split by percentage, idx ordered by train, val, test\n",
    "        train_index = list(range(int(len(data) * dataset_args['train_split'])))\n",
    "        val_index = list(range(int(len(data) * dataset_args['train_split']), int(len(data) * (dataset_args['train_split'] + dataset_args['val_split']))))\n",
    "        test_index = list(range(int(len(data) * (dataset_args['train_split'] + dataset_args['val_split'])), len(data)))\n",
    "        splits.append((train_index, val_index, test_index))\n",
    "\n",
    "    # time series split\n",
    "    else:\n",
    "        testing_pct = 1 - dataset_args['train_split']\n",
    "        tscv = TimeSeriesSplit(n_splits=dataset_args['n_splits'], test_size=testing_pct)\n",
    "        for train_index, test_index in tscv.split(data):\n",
    "            val_index = test_index[:int(len(test_index) * dataset_args['val_split'])]\n",
    "            test_index = test_index[int(len(test_index) * dataset_args['val_split']):]\n",
    "            splits.append((train_index, val_index, test_index))\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "def train_model(data, dataset_args, model_args):\n",
    "\n",
    "\n",
    "    splits = handle_splits(data, dataset_args)\n",
    "    scalers = []\n",
    "    # TODO: only one? check how this works\n",
    "    for train_index, val_index, test_index in tqdm(splits, desc='Cross-validation', total=len(splits)):\n",
    "        # prepare data\n",
    "        train_dataset, val_dataset, test_dataset, scaler = prepare_data(data, train_index, val_index, test_index, dataset_args['sequence_length'])\n",
    "        scalers.append(scaler)\n",
    "\n",
    "        # create loader\n",
    "        train_loader = DataLoader(train_dataset, batch_size=dataset_args['batch_size'])\n",
    "        val_loader = DataLoader(val_dataset, batch_size=dataset_args['batch_size'])\n",
    "        test_loader = DataLoader(test_dataset, batch_size=dataset_args['batch_size'])\n",
    "\n",
    "        raise ValueError(\"Not implemented yet\")\n",
    "        # create model\n",
    "        model = SensorLSTM(input_size=model_args['input_size'], output_size=model_args['output_size'], \n",
    "            hidden_size=model_args['hidden_size'], num_layers=model_args['num_layers'])\n",
    "\n",
    "\n",
    "        # train model\n",
    "        # TODO: training the lstm model, input output, predicitng 1 step ahead, implemented correctly???\n",
    "        trainer = pl.Trainer(max_epochs=model_args['max_epochs'])\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    return model, splits, scalers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_args = {\n",
    "    # define\n",
    "    'sensor': ['G80F11B_Level1'],\n",
    "\n",
    "    # dataset\n",
    "    'n_splits': 1,\n",
    "    'train_split': 0.7, \n",
    "    'val_split': 0.2,\n",
    "    'test_split': 0.1,\n",
    "\n",
    "    # model\n",
    "    'batch_size': 64,\n",
    "    'sequence_length': 50,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    'input_size': (len(dataset_args['sensor']), dataset_args['sequence_length']),\n",
    "    'output_size': len(dataset_args['sensor']),\n",
    "    'hidden_size': 50,\n",
    "    'num_layers': 2,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 10,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"combined_data/clean\", columns=dataset_args['sensor'])\n",
    "data = remove_nans_from_start_end(data, columns=data.columns)\n",
    "\n",
    "# reset index\n",
    "timestamps = data.index\n",
    "data = data.reset_index(drop=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(956975, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Not implemented yet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, splits \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(data, dataset_args, model_args)\u001b[0m\n\u001b[1;32m     36\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mdataset_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     37\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mdataset_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot implemented yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# create model\u001b[39;00m\n\u001b[1;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m SensorLSTM(input_size\u001b[38;5;241m=\u001b[39mmodel_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m'\u001b[39m], output_size\u001b[38;5;241m=\u001b[39mmodel_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     42\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39mmodel_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m], num_layers\u001b[38;5;241m=\u001b[39mmodel_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: Not implemented yet"
     ]
    }
   ],
   "source": [
    "model, splits = train_model(data, dataset_args, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "- Evaluate the model on the test set\n",
    "- Plot the predictions\n",
    "- Calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# OLD:\n",
    "\n",
    "### Considerations:\n",
    "\n",
    "\n",
    "- **Prediction horizon**: \n",
    "    - *1-step-ahead*: focusing on anomaly detection, a 1-step-ahead prediction horizon is chosen for its simplicity.\n",
    "    - *considered*:\n",
    "      - *Multi-step-ahead*: predict multiple steps ahead and compare the predictions with the actual values to detect anomalies. This approach may be more accurate but also more complex.\n",
    "- **Model expandability**:\n",
    "    - *Input*: simply increase the number of input features to include data from multiple sensors.\n",
    "      - *considered*: \n",
    "          - Multiple Models: train a separate model for each sensor and combine their predictions for anomaly detection. But this approach may be less efficient,harder to manage and won't capture interactions between sensors.\n",
    "- **Rain data**:\n",
    "    - *In Output and Learned*: If rain data is a critical factor in detecting anomalies, include it in the output and allow the model to learn its patterns. This approach can help the model differentiate between anomalies caused by rain and other factors. Learn its pattern, then better to predict the anomalies.\n",
    "    - *considered*:\n",
    "        - Not in Output: If rain data is not directly related to the anomalies you're interested in, you might exclude it from the output.\n",
    "        - In Output but Masked in Loss: If rain data affects the system but should not be considered an anomaly, you can include it in the output but mask it in the loss function. This way, the model learns to predict rain data without penalizing deviations.\n",
    "\n",
    "- **Missing data**: mask it in the loss function? what values should it have as input?\n",
    "    - *0.1-1 Range and Masking in Loss*: Normalize the data to a range of 0.1-1 and impute missing values with 0. Then, mask the missing values in the loss function so that the model doesn't penalize them.\n",
    "    - *considered*:\n",
    "        - *Imputing*: keeping 0-1 range but impute with e.g. mean or -1\n",
    "        - *Indicator*: add an indicator feature that specifies whether the value is missing or not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
