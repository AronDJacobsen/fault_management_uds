{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "\n",
    "- For each sensor\n",
    "    - For wet and dry periods\n",
    "        - Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-02 18:58:45.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfault_management_uds.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mPROJ_ROOT path is: /work3/s194262/GitHub/fault_management_uds\u001b[0m\n",
      "2025-03-02 18:58:53.869408: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-02 18:58:54.432654: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740938334.536128 1574332 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740938334.580868 1574332 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-02 18:58:54.910963: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import nexusformat.nexus as nx\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from fault_management_uds.data.hdf_tools import print_tree, load_dataframe_from_HDF5\n",
    "from fault_management_uds.data.process import remove_nans_from_start_end\n",
    "\n",
    "from fault_management_uds.modelling.classifiers import classify_rain_events\n",
    "from fault_management_uds.plots import get_segment_start_end_color, set_meaningful_xticks\n",
    "from fault_management_uds.plots import visualize_error_span\n",
    "from fault_management_uds.config import indicator_2_meta, bools_2_meta, error_indicators, natural_sensor_order\n",
    "\n",
    "\n",
    "from fault_management_uds.config import PROJ_ROOT\n",
    "from fault_management_uds.config import DATA_DIR, RAW_DATA_DIR, INTERIM_DATA_DIR, PROCESSED_DATA_DIR, EXTERNAL_DATA_DIR\n",
    "from fault_management_uds.config import MODELS_DIR, REPORTS_DIR, FIGURES_DIR, REFERENCE_DIR\n",
    "\n",
    "\n",
    "from fault_management_uds.data.load import import_external_metadata, import_metadata\n",
    "from fault_management_uds.data.load import load_data_period, filenames_based_on_period, provided_2_full_range, get_event\n",
    "from fault_management_uds.data.format import create_individual_indicators, create_indicator\n",
    "\n",
    "\n",
    "from fault_management_uds.synthetic.synthetic_generator import AnomalyHandler\n",
    "\n",
    "\n",
    "# set random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = PROCESSED_DATA_DIR / 'Bellinge.h5'\n",
    "external_metadata = import_metadata(REFERENCE_DIR / 'external_metadata.csv')\n",
    "metadata = import_metadata(REFERENCE_DIR / 'sensor_metadata.csv')\n",
    "\n",
    "# Raw sensor path\n",
    "raw_sensor_path = RAW_DATA_DIR / 'Bellinge' / 'sensor-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure save folder\n",
    "figure_save_folder = FIGURES_DIR / 'synthetic'\n",
    "figure_save_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dry and wet periods for each sensor\n",
    "\n",
    "- We want non-erroneous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the classified rain events\n",
    "# clf_rain_events = pd.read_csv(REFERENCE_DIR / 'evetns' / 'rain_events.csv', index_col=0)\n",
    "# clf_rain_events['start'] = pd.to_datetime(clf_rain_events['start'])\n",
    "# clf_rain_events['end'] = pd.to_datetime(clf_rain_events['end'])\n",
    "\n",
    "# sub_rain_events = clf_rain_events[clf_rain_events['duration'] < 60].copy()\n",
    "# # sort by total rain\n",
    "# sub_rain_events.sort_values('total_rain', ascending=False, inplace=True)\n",
    "# sub_rain_events.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dry_wet_periods = {}\n",
    "\n",
    "# # create a dry event dataframe that goes from rain event end to next rain event start\n",
    "# dry_events = []\n",
    "# # sort clf_rain_events by start\n",
    "# clf_rain_events.sort_values('start', inplace=True)\n",
    "# for i in range(clf_rain_events.shape[0] - 1):\n",
    "#     end = clf_rain_events.loc[i, 'end']\n",
    "#     start = clf_rain_events.loc[i+1, 'start']\n",
    "#     duration = (start - end).total_seconds() / 60\n",
    "#     dry_events.append({'start': end, 'end': start, 'duration': duration})\n",
    "\n",
    "# dry_events = pd.DataFrame(dry_events)\n",
    "# # sort by duration\n",
    "# dry_events.sort_values('duration', ascending=False, inplace=True)\n",
    "# dry_events.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# for sensor_name in natural_sensor_order:\n",
    "#     print(f\"Sensor: {sensor_name}\")\n",
    "#     dry_wet_periods[sensor_name] = {}\n",
    "#     # load the data\n",
    "#     data, _, _, _ = load_dataframe_from_HDF5(data_file_path, f\"single_series/sewer_data/{sensor_name}/{sensor_name}_clean\")\n",
    "#     data = remove_nans_from_start_end(data, 'value')\n",
    "    \n",
    "    \n",
    "#     ### Find a rain event\n",
    "#     dry_wet_periods[sensor_name]['rain_event'] = {}\n",
    "#     # filter rain events given sensor time range\n",
    "#     start, end = data.index[0], data.index[-1]\n",
    "#     sensor_rain_events = sub_rain_events[(sub_rain_events['start'] < end) & (sub_rain_events['end'] > start)].copy()\n",
    "#     # iterate rain events until one is found, where the data is not missing\n",
    "#     for i, rain_event in sensor_rain_events.iterrows():\n",
    "#         # get the data for the rain event\n",
    "#         data_rain_period = data.loc[rain_event['start']:rain_event['end']]\n",
    "#         # check if the data is missing\n",
    "#         if data_rain_period['value'].isna().sum() == 0:\n",
    "#             print(f\"    Found rain event on attempt {i+1}\")\n",
    "#             # round up and down to nearest half hour\n",
    "#             dry_wet_periods[sensor_name]['rain_event']['start'] = rain_event['start'].replace(minute=0, second=0, microsecond=0)\n",
    "#             dry_wet_periods[sensor_name]['rain_event']['end'] = rain_event['start'].replace(minute=0, second=0, microsecond=0) + timedelta(hours=2)\n",
    "#             break\n",
    "#     # this else will only be executed if the for loop is not broken\n",
    "#     else:\n",
    "#         print(\"    No rain event found\")\n",
    "#         dry_wet_periods[sensor_name]['rain_event']['start'] = None\n",
    "#         dry_wet_periods[sensor_name]['rain_event']['end'] = None\n",
    "\n",
    "\n",
    "#     ### Find a dry period\n",
    "#     dry_wet_periods[sensor_name]['dry_period'] = {}\n",
    "#     # filter out dry events given sensor time range\n",
    "#     sensor_dry_events = dry_events[(dry_events['start'] < end) & (dry_events['end'] > start)].copy()\n",
    "#     # iterate dry events until one is found, where the data is not missing\n",
    "#     for i, dry_event in sensor_dry_events.iterrows():\n",
    "#         # get the data for the dry event\n",
    "#         data_dry_period = data.loc[dry_event['start']:dry_event['end']]\n",
    "#         # check if the data is missing\n",
    "#         if data_dry_period['value'].isna().sum() == 0:\n",
    "#             print(f\"    Found dry event on attempt {i+1}\")\n",
    "#             dry_wet_periods[sensor_name]['dry_period']['start'] = dry_event['start'].replace(minute=0, second=0, microsecond=0)\n",
    "#             dry_wet_periods[sensor_name]['dry_period']['end'] = dry_event['start'].replace(minute=0, second=0, microsecond=0) + timedelta(hours=2)\n",
    "#             break\n",
    "\n",
    "#     # this else will only be executed if the for loop is not broken\n",
    "#     else:\n",
    "#         print(\"    No dry event found\")\n",
    "#         dry_wet_periods[sensor_name]['dry_period']['start'] = None\n",
    "#         dry_wet_periods[sensor_name]['dry_period']['end'] = None\n",
    "\n",
    "# del data\n",
    "# # save the sensor dry wet periods\n",
    "# with open(REFERENCE_DIR / 'events' / 'dry_wet_periods.json', 'w') as f:\n",
    "#     json.dump(dry_wet_periods, f, default=str, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dry and wet periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sensor dry wet periods\n",
    "with open(REFERENCE_DIR / 'events' / 'dry_wet_periods.json', 'r') as f:\n",
    "    dry_wet_periods = json.load(f)\n",
    "    # convert the start and end times to datetime\n",
    "    for sensor_name in dry_wet_periods.keys():\n",
    "        for event_type in dry_wet_periods[sensor_name].keys():\n",
    "            for event in dry_wet_periods[sensor_name][event_type].keys():\n",
    "                dry_wet_periods[sensor_name][event_type][event] = pd.to_datetime(dry_wet_periods[sensor_name][event_type][event])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sensor ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sensor ranges\n",
    "with open(REFERENCE_DIR / 'sensor_ranges.json', 'r') as f:\n",
    "    sensor_ranges = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml file\n",
    "with open(REFERENCE_DIR / 'synthetic_config.yaml', 'r') as f:\n",
    "    synthetic_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualize anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(sensor_name, period_type, dry_wet_periods, buffer=None):\n",
    "\n",
    "    # get the start and end times\n",
    "    start = dry_wet_periods[sensor_name][period_type]['start']\n",
    "    end = dry_wet_periods[sensor_name][period_type]['end']\n",
    "\n",
    "    if buffer == \"day\":\n",
    "        # round start to start of day and end to end of day\n",
    "        start = start.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        # end is then the next day\n",
    "        end = start + timedelta(days=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # load the event data\n",
    "    rain_5425, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5425\", starttime=start, endtime=end, complete_range=True, verbose=True)\n",
    "    rain_5427, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5427\", starttime=start, endtime=end, complete_range=True, verbose=True)\n",
    "    sensor_data, _, _, _ = load_dataframe_from_HDF5(data_file_path, f\"single_series/sewer_data/{sensor_name}/clean\", starttime=start, endtime=end, complete_range=True, verbose=True)\n",
    "\n",
    "\n",
    "    data_dict = {\n",
    "        '5425': rain_5425,\n",
    "        '5427': rain_5427,\n",
    "        'original': sensor_data,\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "    }\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n",
    "def insert_anomaly(data_dict, anomaly_config, anomaly, sensor_scale, obvious_min, obvious_max, seed, center=False):\n",
    "    # create an anomaly handler\n",
    "    n_obs = data_dict['original'].shape[0]\n",
    "    anomaly_handler = AnomalyHandler(anomaly_config, anomaly, 'value', n_obs, sensor_scale, obvious_min, obvious_max, seed)\n",
    "    \n",
    "    # handling these example cases\n",
    "    # these can have multiple injections\n",
    "    if center:\n",
    "        # set start index to be in the middle of the time series\n",
    "        start_idx = n_obs*2 // 5\n",
    "        anomaly_handler.set_injection_start(start_idx)\n",
    "    else:\n",
    "        anomaly_handler.initialize_injections()\n",
    "\n",
    "    polluted_sensor = anomaly_handler.inject_anomalies(data_dict['original'])\n",
    "    data_dict['polluted'] = polluted_sensor\n",
    "    data_dict['indicator_dict'] = {\n",
    "        'indicator': anomaly_handler.get_indicator(),\n",
    "        'colormap': {\n",
    "            0: 'none',\n",
    "            1: 'firebrick',\n",
    "        }\n",
    "    }\n",
    "    return data_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rain(ax, title, data_dict, marker='o', linewidth=1):\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ### Visualize rain data\n",
    "    for rain_gauge, rain_color in zip(['5427', '5425'], ['purple', 'darkblue']):\n",
    "        ax.plot(data_dict[rain_gauge].index, data_dict[rain_gauge].value, \n",
    "            label=f'Rain gauge {rain_gauge}', color=rain_color,\n",
    "            linewidth=linewidth, linestyle='-', \n",
    "            marker=marker, markersize=1, alpha=1)\n",
    "        ax.set_ylabel('Rain (mm)')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.set_xticks([])\n",
    "        # set y limits based on 0 and max wrt both\n",
    "        ax.set_ylim(-1, data_dict['max_rain'])  \n",
    "        ax.set_xlim(data_dict['start'], data_dict['end'])\n",
    "    return ax\n",
    "\n",
    "\n",
    "def visualize_injected_synthetics(ax, title, data_dict, unit, marker):\n",
    "\n",
    "    # visualize the sensor data; severities\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.plot(data_dict['original'].index, data_dict['original'].value, \n",
    "        label='Original data', color='grey', \n",
    "        linewidth=1, linestyle='-', \n",
    "        marker='', markersize=1, alpha=1)\n",
    "\n",
    "    ax.plot(data_dict['polluted'].index, data_dict['polluted'].value, \n",
    "        label='Erroneous data', color='grey', \n",
    "        linewidth=2, linestyle='-', \n",
    "        marker=marker, markersize=2, alpha=1)\n",
    "    # visualzie error span\n",
    "    ax = visualize_error_span(ax, data_dict['indicator_dict'], data_dict['start'], data_dict['end'], adjust='full-point')\n",
    "    ax.set_xlim(data_dict['start'], data_dict['end'])\n",
    "    ax.set_ylabel(unit)\n",
    "    ax.legend()\n",
    "    ax.set_xticks([])\n",
    "    return ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder\n",
    "examples_save_folder = figure_save_folder / 'examples'\n",
    "examples_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "anomalies = list(synthetic_config['anomalies'].keys())\n",
    "severity = \"medium\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:43<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sensor_name in tqdm(natural_sensor_order, total=len(natural_sensor_order)):\n",
    "    if sensor_name != 'G71F04R_Level2':\n",
    "        continue\n",
    "\n",
    "    # create a folder for the sensor\n",
    "    sensor_save_folder = examples_save_folder / sensor_name\n",
    "    sensor_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    # extract meta\n",
    "    sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "    sensor_meta = metadata[metadata['IdMeasurement'] == sensor_name]\n",
    "    unit = sensor_meta['UnitAlias'].values[0]\n",
    "    obvious_min = sensor_meta['obvious_min'].values[0]\n",
    "    obvious_max = sensor_meta['obvious_max'].values[0]  \n",
    "\n",
    "\n",
    "    # plot a short and long period\n",
    "    #for buffer in [0, 1080]:\n",
    "    for buffer in ['period', 'day']:\n",
    "        marker = \"o\" if buffer == 'period' else \"\"\n",
    "\n",
    "        dry_data_dict = get_data_dict(sensor_name, 'dry_period', dry_wet_periods, buffer=buffer)\n",
    "        wet_data_dict = get_data_dict(sensor_name, 'rain_event', dry_wet_periods, buffer=buffer)\n",
    "\n",
    "        abs_max_rain = max([dry_data_dict['5425'].value.max(), dry_data_dict['5427'].value.max(), wet_data_dict['5425'].value.max(), wet_data_dict['5427'].value.max()]) + 1\n",
    "        dry_data_dict['max_rain'] = abs_max_rain    \n",
    "        wet_data_dict['max_rain'] = abs_max_rain\n",
    "\n",
    "        ### Visualizing all anomalies in one\n",
    "        fig, axs = plt.subplots(1+len(anomalies), 2, figsize=(14, 14), sharey=\"row\")\n",
    "        # set main title\n",
    "        fig.suptitle(f\"{sensor_name}\\n{buffer.capitalize()}\", fontsize=24)\n",
    "        axs[0, 0] = visualize_rain(axs[0, 0], 'Dry period', dry_data_dict, marker)\n",
    "        axs[0, 1] = visualize_rain(axs[0, 1], 'Rain event', wet_data_dict, marker)\n",
    "        # iterate over the anomalies\n",
    "        for i, anomaly in enumerate(anomalies):\n",
    "            anomaly_config = synthetic_config['anomalies'][anomaly][severity]\n",
    "\n",
    "            dry_data_dict = insert_anomaly(dry_data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=True)\n",
    "            wet_data_dict = insert_anomaly(wet_data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=True)\n",
    "            \n",
    "            # Visualize the injected anomalies\n",
    "            sensor_title = f\"{anomaly.capitalize()}\"\n",
    "            axs[i+1, 0] = visualize_injected_synthetics(axs[i+1, 0], sensor_title, dry_data_dict, unit, marker)\n",
    "            axs[i+1, 1] = visualize_injected_synthetics(axs[i+1, 1], sensor_title, wet_data_dict, unit, marker)\n",
    "            \n",
    "\n",
    "        axs[-1, 0] = set_meaningful_xticks(axs[-1, 0], dry_data_dict['start'], dry_data_dict['end'])\n",
    "        axs[-1, 1] = set_meaningful_xticks(axs[-1, 1], wet_data_dict['start'], wet_data_dict['end'])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # # save the figure\n",
    "        fig.savefig(sensor_save_folder / f\"all_{buffer}.png\", dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "        #continue\n",
    "\n",
    "\n",
    "        ### Visualizing each anomaly\n",
    "        # iterate over the anomalies\n",
    "        for anomaly in anomalies:\n",
    "            anomaly_config = synthetic_config['anomalies'][anomaly][severity]\n",
    "            # using standard deviation as the range\n",
    "            #sensor_range = sensor_ranges[sensor_name]['clean']['std']\n",
    "            sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "\n",
    "\n",
    "            dry_data_dict = insert_anomaly(dry_data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=True)\n",
    "            wet_data_dict = insert_anomaly(wet_data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=True)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(16, 6), sharey=\"row\", height_ratios=[1, 2])\n",
    "            # set main title\n",
    "            fig.suptitle(f\"{sensor_name}\\n{anomaly.capitalize()}\", fontsize=24)\n",
    "            sensor_title = f\"{severity.capitalize()} severity\"\n",
    "            axs[0, 0] = visualize_rain(axs[0, 0], 'Dry period', dry_data_dict, marker)\n",
    "            axs[1, 0] = visualize_injected_synthetics(axs[1, 0], sensor_title, dry_data_dict, unit, marker)\n",
    "            axs[-1, 0] = set_meaningful_xticks(axs[-1, 0], dry_data_dict['start'], dry_data_dict['end'])\n",
    "\n",
    "            axs[0, 1] = visualize_rain(axs[0, 1], 'Rain event', wet_data_dict, marker)\n",
    "            axs[1, 1] = visualize_injected_synthetics(axs[1, 1], sensor_title, wet_data_dict, unit, marker)\n",
    "            axs[-1, 1] = set_meaningful_xticks(axs[-1, 1], wet_data_dict['start'], wet_data_dict['end'])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            # save the figure\n",
    "            fig.savefig(sensor_save_folder / f\"{buffer}_{anomaly}.png\", dpi=150)\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop here\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \n",
      "\u001b[0;31mValueError\u001b[0m: Stop here"
     ]
    }
   ],
   "source": [
    "raise ValueError(\"Stop here\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize full timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_name = 'G80F11B_Level1'\n",
    "unit = metadata[metadata['IdMeasurement'] == sensor_name]['UnitAlias'].values[0]\n",
    "\n",
    "# create a folder for the sensor\n",
    "sensor_save_folder = examples_save_folder / sensor_name\n",
    "sensor_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# extract meta\n",
    "sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "sensor_meta = metadata[metadata['IdMeasurement'] == sensor_name]\n",
    "unit = sensor_meta['UnitAlias'].values[0]\n",
    "obvious_min = sensor_meta['obvious_min'].values[0]\n",
    "obvious_max = sensor_meta['obvious_max'].values[0]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full data\n",
    "sensor_data, _, _, _ = load_dataframe_from_HDF5(data_file_path, f\"single_series/sewer_data/{sensor_name}/clean\")\n",
    "\n",
    "start, end = sensor_data.index[0], sensor_data.index[-1]\n",
    "\n",
    "# load the event data\n",
    "rain_5425, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5425\", starttime=start, endtime=end, complete_range=True)\n",
    "rain_5427, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5427\", starttime=start, endtime=end, complete_range=True)\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    '5425': rain_5425,\n",
    "    '5427': rain_5427,\n",
    "    'original': sensor_data,\n",
    "    'start': start,\n",
    "    'end': end,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_injected_synthetics(ax, title, data_dict, unit, marker):\n",
    "\n",
    "    # visualize the sensor data; severities\n",
    "    ax.set_title(title, fontsize=20)\n",
    "\n",
    "    ax.plot(data_dict['polluted'].index, data_dict['polluted'].value, \n",
    "        label='Erroneous data', color='grey', \n",
    "        linewidth=0.3, linestyle='-', \n",
    "        marker=marker, markersize=1, alpha=1)\n",
    "    # visualzie error span\n",
    "    ax = visualize_error_span(ax, data_dict['indicator_dict'], data_dict['start'], data_dict['end'], adjust=60*3, alpha=0.5)\n",
    "    ax.set_xlim(data_dict['start'], data_dict['end'])\n",
    "    ax.set_ylabel(unit)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xticks([])\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain done\n",
      "Total injections: 212\n",
      "spike done: 0.0002999033412576086\n",
      "Total injections: 43\n",
      "noise done: 0.0020470754199430497\n",
      "Total injections: 18\n",
      "frozen done: 0.01017581441521461\n",
      "Total injections: 26\n",
      "offset done: 0.020176075655058908\n",
      "Total injections: 28\n",
      "drift done: 0.020631677943519947\n"
     ]
    }
   ],
   "source": [
    "marker = \"\"\n",
    "\n",
    "abs_max_rain = max([data_dict['5425'].value.max(), data_dict['5427'].value.max()]) + 1\n",
    "data_dict['max_rain'] = abs_max_rain    \n",
    "\n",
    "### Visualizing all anomalies in one\n",
    "fig, axs = plt.subplots(1+len(anomalies), 1, figsize=(14, 14), sharey=\"row\", dpi=150)\n",
    "# set main title\n",
    "fig.suptitle(f\"{sensor_name}\\n{' '}\", fontsize=24)\n",
    "axs[0] = visualize_rain(axs[0], 'Rain', data_dict, marker, linewidth=0.3)\n",
    "print('Rain done')\n",
    "# iterate over the anomalies\n",
    "for i, anomaly in enumerate(anomalies):\n",
    "    anomaly_config = synthetic_config['anomalies'][anomaly][severity]\n",
    "    sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "\n",
    "    data_dict = insert_anomaly(data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=False)\n",
    "    \n",
    "    # Visualize the injected anomalies\n",
    "    sensor_title = f\"{anomaly.capitalize()}\"\n",
    "    axs[i+1] = visualize_injected_synthetics(axs[i+1], sensor_title, data_dict, unit, marker)\n",
    "\n",
    "    print(f\"{anomaly} done: {data_dict['indicator_dict']['indicator'].sum() / data_dict['indicator_dict']['indicator'].shape[0]}\")\n",
    "\n",
    "    \n",
    "\n",
    "axs[-1] = set_meaningful_xticks(axs[-1], data_dict['start'], data_dict['end'])\n",
    "plt.tight_layout()\n",
    "# # save the figure\n",
    "fig.savefig(sensor_save_folder / f\"all_complete.png\", dpi=150)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined approach\n",
    "\n",
    "- not be more than 1-5% of the data\n",
    "- minimum gap of 12 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml file\n",
    "with open(REFERENCE_DIR / 'synthetic_config.yaml', 'r') as f:\n",
    "    synthetic_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder\n",
    "examples_save_folder = figure_save_folder / 'combined'\n",
    "examples_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "anomalies = list(synthetic_config['anomalies'].keys())\n",
    "severity = \"medium\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_name = 'G71F04R_Level2'\n",
    "unit = metadata[metadata['IdMeasurement'] == sensor_name]['UnitAlias'].values[0]\n",
    "\n",
    "# create a folder for the sensor\n",
    "sensor_save_folder = examples_save_folder / sensor_name\n",
    "sensor_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# extract meta\n",
    "sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "sensor_meta = metadata[metadata['IdMeasurement'] == sensor_name]\n",
    "unit = sensor_meta['UnitAlias'].values[0]\n",
    "obvious_min = sensor_meta['obvious_min'].values[0]\n",
    "obvious_max = sensor_meta['obvious_max'].values[0]  \n",
    "\n",
    "# load the full data\n",
    "sensor_data, _, _, _ = load_dataframe_from_HDF5(data_file_path, f\"single_series/sewer_data/{sensor_name}/clean\")\n",
    "\n",
    "# filter to only by year 2013\n",
    "#sensor_data = sensor_data.loc['2013']\n",
    "# filter be be from from start 2012 to middle of 2012\n",
    "sensor_data = sensor_data.loc['2013':'2013-06-01']\n",
    "\n",
    "start, end = sensor_data.index[0], sensor_data.index[-1]\n",
    "\n",
    "\n",
    "# load the event data\n",
    "rain_5425, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5425\", starttime=start, endtime=end, complete_range=True)\n",
    "rain_5427, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5427\", starttime=start, endtime=end, complete_range=True)\n",
    "\n",
    "data_dict = {\n",
    "    '5425': rain_5425,\n",
    "    '5427': rain_5427,\n",
    "    'original': sensor_data,\n",
    "    'start': start,\n",
    "    'end': end,\n",
    "}\n",
    "\n",
    "abs_max_rain = max([data_dict['5425'].value.max(), data_dict['5427'].value.max()]) + 1\n",
    "data_dict['max_rain'] = abs_max_rain    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proportion: 0.0523\n",
      "Days of observations: 152.0\n",
      "Days of anomalies: 7.9496\n"
     ]
    }
   ],
   "source": [
    "total_proportion = 0\n",
    "for anomaly in anomalies:\n",
    "    total_proportion += synthetic_config['anomalies'][anomaly][severity]['proportion']\n",
    "\n",
    "print(f\"Total proportion: {total_proportion}\")\n",
    "n_obs = sensor_data.shape[0]\n",
    "\n",
    "print(f\"Days of observations: {n_obs / (24*60)}\")\n",
    "print(f\"Days of anomalies: {n_obs / (24*60) * total_proportion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_anomaly(data_dict, anomaly_config, anomaly, sensor_scale, obvious_min, obvious_max, seed, \n",
    "                   buffer=12*60, \n",
    "                   edge_buffer=7*24*60,\n",
    "                   center=False, \n",
    "                   available_indices=None,\n",
    "                   n_obs=None\n",
    "):\n",
    "    # create an anomaly handler\n",
    "    n_obs = data_dict['original'].shape[0] if n_obs is None else n_obs\n",
    "    anomaly_handler = AnomalyHandler(anomaly_config, anomaly, 'value', n_obs, sensor_scale, obvious_min, obvious_max, \n",
    "                                     seed=seed, buffer=buffer, edge_buffer=edge_buffer, available_indices=available_indices)\n",
    "    \n",
    "    # handling these example cases\n",
    "    # these can have multiple injections\n",
    "    if center:\n",
    "        # set start index to be in the middle of the time series\n",
    "        start_idx = n_obs*2 // 5\n",
    "        anomaly_handler.set_injection_start(start_idx)\n",
    "    else:\n",
    "        anomaly_handler.initialize_injections()\n",
    "\n",
    "    polluted_sensor = anomaly_handler.inject_anomalies(data_dict['original'])\n",
    "    data_dict['polluted'] = polluted_sensor\n",
    "    data_dict['indicator_dict'] = {\n",
    "        'indicator': anomaly_handler.get_indicator(),\n",
    "        'colormap': {\n",
    "            0: 'none',\n",
    "            #1: 'firebrick',\n",
    "            1: 'lightcoral',\n",
    "\n",
    "        }\n",
    "    }\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input \n",
    "arr1 = [1, 3, False, 4] \n",
    "arr2 = [3, 0, True, False] \n",
    "  \n",
    "# output \n",
    "out_arr = np.logical_or(arr1, arr2) \n",
    "out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly: spike\n",
      "Total injections: 48\n",
      "Edge buffer is in available indices.\n",
      "Last index is in available indices.\n",
      "\n",
      "Anomaly: noise\n",
      "Total injections: 9\n",
      "Edge buffer is in available indices.\n",
      "Last index is in available indices.\n",
      "\n",
      "Anomaly: frozen\n",
      "Total injections: 5\n",
      "Edge buffer is in available indices.\n",
      "Last index is in available indices.\n",
      "\n",
      "Anomaly: offset\n",
      "Total injections: 7\n",
      "Edge buffer is in available indices.\n",
      "Last index is in available indices.\n",
      "\n",
      "Anomaly: drift\n",
      "Total injections: 7\n",
      "Edge buffer is in available indices.\n",
      "Last index is in available indices.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inejct anomalies randomly and iteratively\n",
    "n_obs = sensor_data.shape[0] # keep the same number of observations wrt the proportion of anomalies\n",
    "valid_indices = [i for i in range(n_obs)]\n",
    "indicators = np.zeros(n_obs)\n",
    "\n",
    "for i, anomaly in enumerate(anomalies):\n",
    "    print(f\"Anomaly: {anomaly}\")\n",
    "    anomaly_config = synthetic_config['anomalies'][anomaly][severity]\n",
    "    sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "\n",
    "    data_dict = insert_anomaly(data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, \n",
    "                               # keep buffers\n",
    "                               center=False,\n",
    "                               available_indices=valid_indices,\n",
    "                               n_obs=n_obs\n",
    "                               )\n",
    "    \n",
    "    # Get the valid indices based on the injected anomalies and the current indicator\n",
    "    anomaly_indicator = data_dict['indicator_dict']['indicator']\n",
    "    # join the indicators\n",
    "    indicators = np.logical_or(indicators, anomaly_indicator) # 0 if both are 0, 1 if one is 1\n",
    "\n",
    "\n",
    "    # Remove indices from valid_indices where the indicator is 1\n",
    "    valid_indices = [i for i in valid_indices if not indicators[i]]\n",
    "\n",
    "    # set the original to the polluted\n",
    "    data_dict['original'] = data_dict['polluted']\n",
    "    print('')\n",
    "\n",
    "# reinsert the original data\n",
    "data_dict['original'] = sensor_data\n",
    "# insert indicators\n",
    "data_dict['indicator_dict'] = {\n",
    "    'indicator': indicators,\n",
    "    'colormap': {\n",
    "        0: 'none',\n",
    "        1: 'lightcoral',\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain done\n"
     ]
    }
   ],
   "source": [
    "marker = \"\"\n",
    "\n",
    "### Visualizing all anomalies in one\n",
    "fig, axs = plt.subplots(2, 1, figsize=(14, 6), sharey=\"row\", dpi=150)\n",
    "# set main title\n",
    "fig.suptitle(f\"{sensor_name}\\n{' '}\", fontsize=24)\n",
    "axs[0] = visualize_rain(axs[0], 'Rain', data_dict, marker, linewidth=0.3)\n",
    "print('Rain done')\n",
    "\n",
    "# Visualize the injected anomalies\n",
    "sensor_title = sensor_name\n",
    "axs[1] = visualize_injected_synthetics(axs[1], sensor_title, data_dict, unit, marker)\n",
    "\n",
    "# set legend location to upper right\n",
    "axs[-1].legend(loc='upper right')\n",
    "    \n",
    "\n",
    "axs[-1] = set_meaningful_xticks(axs[-1], data_dict['start'], data_dict['end'])\n",
    "plt.tight_layout()\n",
    "# save the figure\n",
    "fig.savefig(examples_save_folder / f\"{sensor_name}_combined.png\", dpi=300)\n",
    "plt.close(fig)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1574332/3521144021.py:22: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(ax.get_yticklabels(), fontsize=15)\n",
      "/tmp/ipykernel_1574332/3521144021.py:22: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(ax.get_yticklabels(), fontsize=15)\n",
      "/tmp/ipykernel_1574332/3521144021.py:23: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(ax.get_xticklabels(), fontsize=15)\n"
     ]
    }
   ],
   "source": [
    "marker = \"\"\n",
    "\n",
    "### Visualizing all anomalies in one\n",
    "fig, axs = plt.subplots(2, 1, figsize=(14, 6), sharey=\"row\", dpi=150)\n",
    "# set main title\n",
    "fig.suptitle(f\"{sensor_name}\\n{' '}\", fontsize=24)\n",
    "axs[0] = visualize_rain(axs[0], 'Rain', data_dict, marker, linewidth=0.3)\n",
    "print('Rain done')\n",
    "\n",
    "# Visualize the injected anomalies\n",
    "sensor_title = sensor_name\n",
    "axs[1] = visualize_injected_synthetics(axs[1], sensor_title, data_dict, unit, marker)\n",
    "\n",
    "# set legend location to upper right\n",
    "axs[-1].legend(loc='upper right')\n",
    "\n",
    "axs[-1] = set_meaningful_xticks(axs[-1], data_dict['start'], data_dict['end'])\n",
    "\n",
    "# increase the size of the y label, y ticks, x ticks, and legends for both plots\n",
    "for ax in axs:\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=16)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=15)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=15)\n",
    "    ax.legend(fontsize=16, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "# save the figure\n",
    "fig.savefig(examples_save_folder / f\"{sensor_name}_combined_year.png\", dpi=500)\n",
    "plt.close(fig)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Todo\n",
    "\n",
    "- compare this real errors?\n",
    "- some synthetic data summary?\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
