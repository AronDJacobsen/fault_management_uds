{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data processing\n",
    "\n",
    "The goal is to:\n",
    "- load the raw data\n",
    "- collect each sensor into a single series\n",
    "- Process the manual removals\n",
    "- then save it within the H5py file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- manual removals\n",
    "- save to the h5py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine a sensors raw data files into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_raw_data_file(path, source, conversion):\n",
    "    if source == 'System2000':\n",
    "        raw_data = import_system2000(path, conversion)\n",
    "    elif source == 'iFix':  \n",
    "        raw_data = import_ifix(path, conversion)\n",
    "    elif source == 'Danova':  \n",
    "        raw_data = import_danova(path, conversion)\n",
    "    else: \n",
    "        print(\"Unknown source, i.e. not System2000, iFix or Danova\")\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def load_raw_data(metadata, sensor_data_path, save_path):\n",
    "    \"\"\"Load raw data files based on metadata.\"\"\"\n",
    "\n",
    "    raw_data_paths = {}\n",
    "\n",
    "    # iterate each sensor group\n",
    "    n_groups = metadata['IdMeasurement'].nunique()\n",
    "    i_group = 0\n",
    "    for sensor_name, sensor_group in metadata.groupby('IdMeasurement'):\n",
    "        print(f\"({i_group+1}/{n_groups}) Loading {sensor_name}\")\n",
    "        # create a dictionary with the sensor id as key and the data as value\n",
    "        raw_sensor_data = pd.DataFrame()\n",
    "        # iterate each row in the sensor group\n",
    "        sensor_group = sensor_group.reset_index(drop=True)\n",
    "        for i, row in sensor_group.iterrows():\n",
    "            print(f\"    {i+1}/{sensor_group.shape[0]}\")\n",
    "            # get the file path\n",
    "            file_path = sensor_data_path / row['Folderpath'] / row['Filename']\n",
    "            # load the raw data file: currently only adjust the datetime column\n",
    "            sensor_data = load_raw_data_file(file_path, row['Source'], row['Conversion'])\n",
    "            # sort by time\n",
    "            sensor_data = sensor_data.sort_values(by='time')\n",
    "            # add the raw data to the df in the dictionary\n",
    "            raw_sensor_data = pd.concat([raw_sensor_data, sensor_data])\n",
    "        # remove duplicated time\n",
    "        raw_sensor_data = raw_sensor_data.drop_duplicates(subset=['time'])\n",
    "        # sort by time\n",
    "        raw_sensor_data = raw_sensor_data.sort_values(by='time')\n",
    "        # remove nan values\n",
    "        raw_sensor_data = raw_sensor_data.dropna(subset=['value'])\n",
    "        # save the raw data as a pickle file\n",
    "        file_path = save_path / f'{sensor_name}.pkl'\n",
    "        raw_sensor_data.to_pickle(file_path)\n",
    "        raw_data_paths[sensor_name] = file_path\n",
    "        i_group += 1\n",
    "        print(f\"Saved {sensor_name} to {save_path / f'{sensor_name}.pkl'}\")\n",
    "        print('')\n",
    "    # save the raw data paths\n",
    "    with open(save_path / 'raw_data_paths.pkl', 'wb') as f:\n",
    "        pickle.dump(raw_data_paths, f)\n",
    "    return raw_data_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNTIME: 20 minutes\n",
    "\n",
    "GOAL: saving av pickle for faster load time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = INTERIM_DATA_DIR / 'Bellinge' / 'sensor-data'\n",
    "# create the save path if it does not exist\n",
    "save_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data_paths = load_raw_data(metadata, sensor_data_path, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the raw data paths\n",
    "with open(save_path / 'raw_data_paths.pkl', 'rb') as f:\n",
    "    raw_data_paths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pre-Processing\n",
    "\n",
    "The goal is to:\n",
    "- Resample data into 1 minute intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- save to the h5py file\n",
    "- make for each individual sensor, not all combined\n",
    "- then make for combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset_data(raw_data_paths, min_time, max_time, resample_freq):\n",
    "    \"\"\"Goal: Create a subset of the raw data based on time and within a single dataframe.\"\"\"\n",
    "    sensor_names = list(raw_data_paths.keys())\n",
    "    # create an dataframe with the time as index\n",
    "    time_range = pd.date_range(start=min_time, end=max_time, freq=resample_freq)\n",
    "    subset_data = pd.DataFrame(index=time_range)\n",
    "    for i, sensor_name in enumerate(sensor_names):\n",
    "        print(f\"({i+1}/{len(sensor_names)}) Loading {sensor_name}\")\n",
    "        raw_data = pd.read_pickle(raw_data_paths[sensor_name])\n",
    "        # make sure time is of the correct type\n",
    "        raw_data['time'] = pd.to_datetime(raw_data['time'])\n",
    "        # set time as index\n",
    "        raw_data = raw_data.set_index('time')\n",
    "        # extract the value column\n",
    "        raw_data = raw_data[['value']]\n",
    "        # rename the value column to the sensor name\n",
    "        raw_data = raw_data.rename(columns={'value': sensor_name})\n",
    "\n",
    "        ### Performing necessary data cleaning (1 minute resampling)   \n",
    "        # Create a new DataFrame with the time range and no data\n",
    "        time_range = pd.date_range(start=raw_data.index.min(), end=raw_data.index.max(), freq='1min')\n",
    "        time_df = pd.DataFrame(index=time_range)\n",
    "        # Concatenate the original raw_data with the new time_df\n",
    "        expanded_data = pd.concat([raw_data, time_df], axis=1) # This will create NaNs for the new time points, which appear after the original data\n",
    "        # Handle duplicate indices (i.e., original data points that already exist in the 1-minute intervals)\n",
    "\n",
    "        # Drop any duplicate indices (i.e., original data points that already exist in the 1-minute intervals)\n",
    "        expanded_data = expanded_data[~expanded_data.index.duplicated(keep='first')]\n",
    "        # Sort the data by time\n",
    "        expanded_data = expanded_data.sort_index()\n",
    "        # Interpolate to fill in the gaps, limiting interpolation to small gaps (e.g., up to 2 missing minutes)\n",
    "        interpolated_data = expanded_data.interpolate(method='time', limit=2)\n",
    "        # Now remove the original irregular time points, keeping only the regular 1-minute intervals\n",
    "        regular_data = interpolated_data.loc[time_range]\n",
    "\n",
    "        # add the data to the subset data based on time\n",
    "        subset_data = pd.concat([subset_data, regular_data], axis=1)\n",
    "\n",
    "    # sort by time\n",
    "    subset_data = subset_data.sort_index()\n",
    "    # save the subset data as a pickle file\n",
    "    subset_data_path = save_path / 'subset_data.pkl'\n",
    "    subset_data.to_pickle(subset_data_path)\n",
    "    return subset_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset of the data\n",
    "min_time = '2020-01-01 00:00:00'\n",
    "max_time = '2020-12-31 23:59:59'\n",
    "# resample seems to be 1 minute\n",
    "resample_freq = '1min'\n",
    "# RUNTIME: 2 minutes\n",
    "# MEMORY: ~80 MB\n",
    "subset_data_path = create_subset_data(raw_data_paths, min_time, max_time, resample_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "## Custom Processing\n",
    "\n",
    "The goal is to:\n",
    "- Errors within the data, use their\n",
    "- Quality comparison with the other processing pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-found outliers/errors\n",
    "- *Manufacturer quality stamp*. These data were stamped with “low quality” in the iFIX SCADA system.\n",
    "- *Manual remove*. These are data that for some reason were deemed untrustworthy, for instance observation values during maintenance or start-up periods.\n",
    "- *Out of bounds*. These are data outside a defined physically meaningful range of possible values (e.g. bottom and top levels of a pipe/basin).\n",
    "- *Frozen sensor*. These data do not change during a time period of e.g. 20 min.\n",
    "- *Outlier*. These are data with spikes with a manually chosen height and duration; in our case this category is only applicable to interim Danova sensor data, which occasionally showed spike patterns which are probably not correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on the pre-defined processing step\n",
    "\n",
    "- The interpolation seems to be ill-defined\n",
    "    - TODO: check how many missing values it can interpolate\n",
    "- Not all files have the frozen_high column\n",
    "- scaling factor comments?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
