{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "\n",
    "- For each sensor\n",
    "    - For wet and dry periods\n",
    "        - Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 10:48:32.431 | INFO     | fault_management_uds.config:<module>:11 - PROJ_ROOT path is: /Users/arond.jacobsen/Documents/GitHub/fault_management_uds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import nexusformat.nexus as nx\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from fault_management_uds.data.hdf_functions import print_tree, load_dataframe_from_HDF5\n",
    "from fault_management_uds.data.process import remove_nans_from_start_end\n",
    "\n",
    "from fault_management_uds.modelling.classifiers import classify_rain_events\n",
    "from fault_management_uds.plots import get_segment_start_end_color, set_meaningful_xticks\n",
    "from fault_management_uds.plots import visualize_error_span\n",
    "from fault_management_uds.config import indicator_2_meta, bools_2_meta, error_indicators, natural_sensor_order\n",
    "\n",
    "\n",
    "from fault_management_uds.config import PROJ_ROOT\n",
    "from fault_management_uds.config import DATA_DIR, RAW_DATA_DIR, INTERIM_DATA_DIR, PROCESSED_DATA_DIR, EXTERNAL_DATA_DIR\n",
    "from fault_management_uds.config import MODELS_DIR, REPORTS_DIR, FIGURES_DIR, REFERENCE_DIR\n",
    "\n",
    "\n",
    "from fault_management_uds.data.load import import_external_metadata, import_metadata\n",
    "from fault_management_uds.data.load import load_data_period, filenames_based_on_period, provided_2_full_range, get_event\n",
    "from fault_management_uds.data.format import create_individual_indicators, create_indicator\n",
    "\n",
    "\n",
    "# set random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = PROCESSED_DATA_DIR / 'Bellinge.h5'\n",
    "external_metadata = import_metadata(REFERENCE_DIR / 'external_metadata.csv')\n",
    "metadata = import_metadata(REFERENCE_DIR / 'sensor_metadata.csv')\n",
    "\n",
    "# Raw sensor path\n",
    "raw_sensor_path = RAW_DATA_DIR / 'Bellinge' / 'sensor-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure save folder\n",
    "figure_save_folder = FIGURES_DIR / 'synthetic'\n",
    "figure_save_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dry and wet periods for each sensor\n",
    "\n",
    "- We want non-erroneous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the classified rain events\n",
    "# clf_rain_events = pd.read_csv(REFERENCE_DIR / 'evetns' / 'rain_events.csv', index_col=0)\n",
    "# clf_rain_events['start'] = pd.to_datetime(clf_rain_events['start'])\n",
    "# clf_rain_events['end'] = pd.to_datetime(clf_rain_events['end'])\n",
    "\n",
    "# sub_rain_events = clf_rain_events[clf_rain_events['duration'] < 60].copy()\n",
    "# # sort by total rain\n",
    "# sub_rain_events.sort_values('total_rain', ascending=False, inplace=True)\n",
    "# sub_rain_events.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dry_wet_periods = {}\n",
    "\n",
    "# # create a dry event dataframe that goes from rain event end to next rain event start\n",
    "# dry_events = []\n",
    "# # sort clf_rain_events by start\n",
    "# clf_rain_events.sort_values('start', inplace=True)\n",
    "# for i in range(clf_rain_events.shape[0] - 1):\n",
    "#     end = clf_rain_events.loc[i, 'end']\n",
    "#     start = clf_rain_events.loc[i+1, 'start']\n",
    "#     duration = (start - end).total_seconds() / 60\n",
    "#     dry_events.append({'start': end, 'end': start, 'duration': duration})\n",
    "\n",
    "# dry_events = pd.DataFrame(dry_events)\n",
    "# # sort by duration\n",
    "# dry_events.sort_values('duration', ascending=False, inplace=True)\n",
    "# dry_events.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# for sensor_name in natural_sensor_order:\n",
    "#     print(f\"Sensor: {sensor_name}\")\n",
    "#     dry_wet_periods[sensor_name] = {}\n",
    "#     # load the data\n",
    "#     data, _, _, _ = load_dataframe_from_HDF5(data_file_path, f\"single_series/sewer_data/{sensor_name}/{sensor_name}_clean\")\n",
    "#     data = remove_nans_from_start_end(data, 'value')\n",
    "    \n",
    "    \n",
    "#     ### Find a rain event\n",
    "#     dry_wet_periods[sensor_name]['rain_event'] = {}\n",
    "#     # filter rain events given sensor time range\n",
    "#     start, end = data.index[0], data.index[-1]\n",
    "#     sensor_rain_events = sub_rain_events[(sub_rain_events['start'] < end) & (sub_rain_events['end'] > start)].copy()\n",
    "#     # iterate rain events until one is found, where the data is not missing\n",
    "#     for i, rain_event in sensor_rain_events.iterrows():\n",
    "#         # get the data for the rain event\n",
    "#         data_rain_period = data.loc[rain_event['start']:rain_event['end']]\n",
    "#         # check if the data is missing\n",
    "#         if data_rain_period['value'].isna().sum() == 0:\n",
    "#             print(f\"    Found rain event on attempt {i+1}\")\n",
    "#             # round up and down to nearest half hour\n",
    "#             dry_wet_periods[sensor_name]['rain_event']['start'] = rain_event['start'].replace(minute=0, second=0, microsecond=0)\n",
    "#             dry_wet_periods[sensor_name]['rain_event']['end'] = rain_event['start'].replace(minute=0, second=0, microsecond=0) + timedelta(hours=2)\n",
    "#             break\n",
    "#     # this else will only be executed if the for loop is not broken\n",
    "#     else:\n",
    "#         print(\"    No rain event found\")\n",
    "#         dry_wet_periods[sensor_name]['rain_event']['start'] = None\n",
    "#         dry_wet_periods[sensor_name]['rain_event']['end'] = None\n",
    "\n",
    "\n",
    "#     ### Find a dry period\n",
    "#     dry_wet_periods[sensor_name]['dry_period'] = {}\n",
    "#     # filter out dry events given sensor time range\n",
    "#     sensor_dry_events = dry_events[(dry_events['start'] < end) & (dry_events['end'] > start)].copy()\n",
    "#     # iterate dry events until one is found, where the data is not missing\n",
    "#     for i, dry_event in sensor_dry_events.iterrows():\n",
    "#         # get the data for the dry event\n",
    "#         data_dry_period = data.loc[dry_event['start']:dry_event['end']]\n",
    "#         # check if the data is missing\n",
    "#         if data_dry_period['value'].isna().sum() == 0:\n",
    "#             print(f\"    Found dry event on attempt {i+1}\")\n",
    "#             dry_wet_periods[sensor_name]['dry_period']['start'] = dry_event['start'].replace(minute=0, second=0, microsecond=0)\n",
    "#             dry_wet_periods[sensor_name]['dry_period']['end'] = dry_event['start'].replace(minute=0, second=0, microsecond=0) + timedelta(hours=2)\n",
    "#             break\n",
    "\n",
    "#     # this else will only be executed if the for loop is not broken\n",
    "#     else:\n",
    "#         print(\"    No dry event found\")\n",
    "#         dry_wet_periods[sensor_name]['dry_period']['start'] = None\n",
    "#         dry_wet_periods[sensor_name]['dry_period']['end'] = None\n",
    "\n",
    "# del data\n",
    "# # save the sensor dry wet periods\n",
    "# with open(REFERENCE_DIR / 'events' / 'dry_wet_periods.json', 'w') as f:\n",
    "#     json.dump(dry_wet_periods, f, default=str, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dry and wet periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sensor dry wet periods\n",
    "with open(REFERENCE_DIR / 'events' / 'dry_wet_periods.json', 'r') as f:\n",
    "    dry_wet_periods = json.load(f)\n",
    "    # convert the start and end times to datetime\n",
    "    for sensor_name in dry_wet_periods.keys():\n",
    "        for event_type in dry_wet_periods[sensor_name].keys():\n",
    "            for event in dry_wet_periods[sensor_name][event_type].keys():\n",
    "                dry_wet_periods[sensor_name][event_type][event] = pd.to_datetime(dry_wet_periods[sensor_name][event_type][event])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sensor ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sensor ranges\n",
    "with open(REFERENCE_DIR / 'sensor_ranges.json', 'r') as f:\n",
    "    sensor_ranges = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml file\n",
    "with open(REFERENCE_DIR / 'synthetic_config.yaml', 'r') as f:\n",
    "    synthetic_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyGenerator:\n",
    "    # no need for init\n",
    "    def inject_anomaly(self, time_series, value_col_idx, anomaly_type, start_idx, end_idx, duration, magnitude):\n",
    "        \"\"\"Inject a specific type of anomaly into the time series.\"\"\"\n",
    "\n",
    "        # Inject the specified anomaly type\n",
    "        if anomaly_type == \"spike\":\n",
    "            time_series = self.inject_spike(time_series, value_col_idx, start_idx, end_idx, duration, magnitude)\n",
    "        elif anomaly_type == \"noise\":\n",
    "            time_series = self.inject_noise(time_series, value_col_idx, start_idx, end_idx, duration, magnitude)\n",
    "        elif anomaly_type == \"frozen\":\n",
    "            time_series = self.inject_frozen(time_series, value_col_idx, start_idx, end_idx, duration, magnitude)\n",
    "        elif anomaly_type == \"offset\":\n",
    "            time_series = self.inject_offset(time_series, value_col_idx, start_idx, end_idx, duration, magnitude)\n",
    "        elif anomaly_type == \"drift\":\n",
    "            time_series = self.inject_drift(time_series, value_col_idx, start_idx, end_idx, duration, magnitude)\n",
    "        \n",
    "        return time_series\n",
    "\n",
    "    def inject_spike(self, time_series, value_col_idx, start_idx, end_idx, duration, magnitude):\n",
    "        \"\"\"Inject a single spike anomaly.\"\"\"\n",
    "\n",
    "        midpoint = duration // 2\n",
    "        # controls 2 things: 1) uneven, increase linspace length, 2) uneven, do not include midpoint in the last half\n",
    "        even_uneven_parameter = duration % 2 # 0 if even, 1 if uneven\n",
    "        first_half = np.linspace(0, magnitude, midpoint+1+even_uneven_parameter)[1:] # don't include the first 0\n",
    "        # if even, include midpoint in the last half\n",
    "        # if uneven, do not include midpoint in the last half\n",
    "        last_half = first_half[::-1][even_uneven_parameter:].copy()\n",
    "        pattern = np.concatenate([first_half, last_half])\n",
    "\n",
    "        time_series.iloc[start_idx:end_idx, value_col_idx] += pattern \n",
    "        return time_series\n",
    "\n",
    "    def inject_noise(self, time_series, value_col_idx, start_idx, end_idx, duration, magnitude):\n",
    "        \"\"\"Inject random noise anomaly over a duration.\"\"\"\n",
    "        noise = np.random.normal(0, magnitude, duration)\n",
    "        time_series.iloc[start_idx:end_idx, value_col_idx] += noise\n",
    "        return time_series\n",
    "\n",
    "    def inject_frozen(self, time_series, value_col_idx, start_idx, end_idx, duration, magnitude):\n",
    "        \"\"\"Inject a frozen anomaly (constant value).\"\"\"\n",
    "        frozen_value = time_series.iloc[start_idx]\n",
    "        time_series.iloc[start_idx:end_idx, value_col_idx] = frozen_value\n",
    "        return time_series\n",
    "\n",
    "    def inject_offset(self, time_series, value_col_idx, start_idx, end_idx, duration, magnitude):\n",
    "        \"\"\"Inject an offset anomaly (shifted baseline over duration).\"\"\"\n",
    "        time_series.iloc[start_idx:end_idx, value_col_idx] += magnitude\n",
    "        return time_series\n",
    "\n",
    "    def inject_drift(self, time_series, value_col_idx, start_idx, end_idx, duration, magnitude):\n",
    "        \"\"\"Inject a drift anomaly (gradual increase over duration).\"\"\"\n",
    "        drift = np.linspace(0, magnitude, end_idx - start_idx)\n",
    "        time_series.iloc[start_idx:end_idx, value_col_idx] += drift\n",
    "        return time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AnomalyHandler:\n",
    "    def __init__(self, anomaly_config, anomaly, value_col, n_obs, magnitude_scale, obvious_min, obvious_max, seed):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        config : dict\n",
    "            Configuration dictionary for the anomalies.\n",
    "        time_series : pd.Series\n",
    "            Time series data to inject anomalies into.\n",
    "        anomaly : str\n",
    "            Type of anomaly to inject.\n",
    "        magnitude_scale : list\n",
    "            Range of magnitudes to inject.\n",
    "        seed : int\n",
    "            Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        # define\n",
    "        self.n_obs = n_obs\n",
    "        self.anomaly = anomaly\n",
    "        self.seed = seed\n",
    "        self.value_col = value_col\n",
    "\n",
    "\n",
    "        # set configuration parameters\n",
    "        self.anomaly_config = anomaly_config\n",
    "        self.proportion = self.anomaly_config['proportion']\n",
    "        # set the magnitude range based on e.g. min-max or std\n",
    "        # - unique for each sensor\n",
    "        self.magnitude_scale = magnitude_scale\n",
    "        self.obvious_min = obvious_min\n",
    "        self.obvious_max = obvious_max\n",
    "\n",
    "        # define duration parameters\n",
    "        self.duration_mean = self.anomaly_config['duration']['normal']['mean']\n",
    "        self.duration_std = self.anomaly_config['duration']['normal']['std']\n",
    "        # define magnitude parameters\n",
    "        self.magnitude_min = self.anomaly_config['magnitude']['range']['min']\n",
    "        self.magnitude_max = self.anomaly_config['magnitude']['range']['max']\n",
    "        self.magnitude_sign = self.anomaly_config['magnitude']['sign']\n",
    "        \n",
    "        # initialize the injections\n",
    "        self.total_injections = 0\n",
    "        self.start_indices = []\n",
    "        self.durations = []\n",
    "        self.magnitudes = []\n",
    "\n",
    "        # store the anomaly generator\n",
    "        self.anomaly_generator = AnomalyGenerator()\n",
    "\n",
    "\n",
    "    def initialize_injections(self):\n",
    "        \"\"\"\n",
    "        Initialize the injections based on the configuration.\n",
    "        - Number of injections are based on the proportion defined and the sampled durations.\n",
    "        \"\"\"\n",
    "\n",
    "        # # set the seed to reset and ensure reproducibility\n",
    "        # np.random.seed(self.seed)\n",
    "\n",
    "        # find the total duration of the anomalies based on the proportion\n",
    "        self.total_duration = round(self.proportion * self.n_obs)\n",
    "\n",
    "        # get the durations until the total duration is reached\n",
    "        self.durations = []\n",
    "        current_duration_sum = 0\n",
    "        while sum(self.durations) < self.total_duration:\n",
    "            # Generate samples\n",
    "            duration_sample = self.sample_duration()\n",
    "            self.durations.append(duration_sample)\n",
    "            current_duration_sum += duration_sample\n",
    "        \n",
    "        # total injections\n",
    "        self.total_injections = len(self.durations)\n",
    "        \n",
    "        # sample the magnitudes\n",
    "        self.magnitudes = self.sample_magnitudes()\n",
    "\n",
    "        # get start indices\n",
    "        self.start_indices = self.get_start_indices()\n",
    "\n",
    "        # TODO: evaluate consistency?\n",
    "\n",
    "\n",
    "    def sample_duration(self):\n",
    "        \"\"\"Sample a duration from a normal distribution.\"\"\"\n",
    "        # Generate samples\n",
    "        sample = np.random.normal(loc=self.duration_mean, scale=self.duration_std, size=1)[0]\n",
    "        # Round to integer and clip the duration to minimum 1 minute\n",
    "        sample = np.clip(int(np.round(sample)), 1, None)\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def sample_magnitudes(self):\n",
    "        \"\"\"\n",
    "        Sample magnitudes from a specified range distribution.\n",
    "        - handle None, rounding, direction\n",
    "        \"\"\"\n",
    "        # If None, for e.g. Frozen\n",
    "        if self.magnitude_min is None or self.magnitude_max is None:\n",
    "            # return an array of 1 (identity), use total_injections\n",
    "            return np.ones(self.total_injections)\n",
    "        \n",
    "        # sample the magnitudes\n",
    "        samples = np.random.uniform(self.magnitude_min, self.magnitude_max, self.total_injections)\n",
    "        \n",
    "        # round to 3 decimals\n",
    "        samples = np.round(samples, 3)\n",
    "        \n",
    "        # handle directions\n",
    "        if self.magnitude_sign == '+':\n",
    "            samples = np.abs(samples)\n",
    "        elif self.magnitude_sign == '-':\n",
    "            samples = -np.abs(samples)\n",
    "        elif self.magnitude_sign == '+-':\n",
    "            # sample from both sides\n",
    "            pos_neg = np.random.choice([-1, 1], self.total_injections)\n",
    "            samples = samples * pos_neg\n",
    "        \n",
    "        # Apply the scale\n",
    "        samples = samples * self.magnitude_scale\n",
    "        return samples\n",
    "\n",
    "\n",
    "    def get_start_indices(self):\n",
    "        \"\"\"\n",
    "        Based on total injections and selected durations, place the start indices.\n",
    "        \n",
    "        Note: \n",
    "        - should be placed somewhat evenly around\n",
    "        - not too close to the start or end\n",
    "        - not too close to each other\n",
    "        \"\"\"\n",
    "\n",
    "        # The minimum gap between anomalies (could be adjusted)\n",
    "        min_gap = 12*60  # This is the minimum number of time steps between anomalies\n",
    "        \n",
    "        # Initialize a list of potential start indices (avoiding the first and last 5% of the time series)\n",
    "        min_idx = 7*24*60\n",
    "        # should not exceed the time series length\n",
    "        max_idx = self.n_obs - min_idx\n",
    "        # Ensure there is enough space to place at least one anomaly\n",
    "        if max_idx - min_idx < min_gap:\n",
    "            print(\"Not enough space to place anomalies with the given constraints.\")\n",
    "            return []\n",
    "            \n",
    "        # Available indices will decrease as anomalies are placed; based on where they can be placed and the min gap\n",
    "        available_indices = list(range(min_idx, max_idx))\n",
    "        max_duration = max(self.durations)\n",
    "\n",
    "        # Randomly pick start indices for the anomalies\n",
    "        start_indices = []\n",
    "\n",
    "        for i in range(int(self.total_injections)):\n",
    "            # Check if there no valid choices left\n",
    "            if not available_indices:\n",
    "                print(f\"No valid choices, breaking early: injected {i} out of {self.total_injections}\")\n",
    "                break\n",
    "\n",
    "            # Randomly select an index\n",
    "            avail_start_idx = random.randint(0, len(available_indices))\n",
    "            start_idx = available_indices[avail_start_idx]\n",
    "            duration = self.durations[i]\n",
    "\n",
    "            # Add the index to the list of start indices\n",
    "            start_indices.append(start_idx)\n",
    "\n",
    "            # Update the list of available indices\n",
    "            del available_indices[avail_start_idx - max_duration - min_gap:avail_start_idx + duration + min_gap]\n",
    "            \n",
    "\n",
    "        # In case of early break, update:\n",
    "        self.total_injections = len(start_indices)\n",
    "        self.durations = [self.durations[i] for i in range(self.total_injections)]\n",
    "        self.magnitudes = [self.magnitudes[i] for i in range(self.total_injections)]\n",
    "        \n",
    "        return start_indices\n",
    "\n",
    "\n",
    "    def set_injection_start(self, start_idx):\n",
    "        # set an injections based on a start index\n",
    "        self.start_indices.append(start_idx)\n",
    "        self.total_injections += 1\n",
    "        self.durations.append(self.sample_duration())\n",
    "        self.magnitudes.append(self.sample_magnitudes()[0])\n",
    "\n",
    "        # adjust in case it exceeds the time series length\n",
    "        if start_idx + self.durations[-1] >= self.n_obs:\n",
    "            # adjust the duration\n",
    "            self.durations[-1] = self.n_obs - start_idx\n",
    "            # if drift, also adjust the magnitude\n",
    "            if self.anomaly in ['drift']:\n",
    "                # adjust the magnitude relative to the duration (so it still follow the same slope)\n",
    "                self.magnitudes[-1] = self.magnitudes[-1] * (self.durations[-1] / self.durations[-1])\n",
    "\n",
    "        # evaluate the consistency to ensure no overlap\n",
    "        self.evaluate_consistency()\n",
    "            \n",
    "\n",
    "    def evaluate_consistency(self, action='remove'):\n",
    "        \"\"\"Evaluate the consistency of the injections.\"\"\"\n",
    "        # sort the start indices\n",
    "        self.start_indices = sorted(self.start_indices)\n",
    "        i = 1\n",
    "        while i < self.total_injections:\n",
    "            # check if the current start index is less than the previous end index\n",
    "            if self.start_indices[i] < self.start_indices[i-1] + self.durations[i-1]:\n",
    "                # overlap detected\n",
    "                print(f\"Overlap detected: {self.start_indices[i-1]}-{self.start_indices[i-1] + self.durations[i-1]} and {self.start_indices[i]}-{self.start_indices[i] + self.durations[i]}\")\n",
    "                if action == 'remove':\n",
    "                    # remove the current injection\n",
    "                    del self.start_indices[i]\n",
    "                    del self.durations[i]\n",
    "                    del self.magnitudes[i]\n",
    "                    self.total_injections -= 1\n",
    "                    # do not increment i, as we need to check the new current element\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    def get_indicator(self):\n",
    "        \"\"\"Return a 0-1 mask for the injected anomalies.\"\"\"\n",
    "        indicator = np.zeros(self.n_obs)\n",
    "        for i in range(self.total_injections):\n",
    "            start_time = self.start_indices[i]\n",
    "            duration = self.durations[i]\n",
    "            indicator[start_time:start_time + duration] = 1\n",
    "        return indicator\n",
    "\n",
    "\n",
    "\n",
    "    def inject_anomalies(self, time_series):\n",
    "        \"\"\"Inject anomalies into the time series based on the configuration.\"\"\"\n",
    "        # make a copy of the time series\n",
    "        self.time_series = time_series.copy()\n",
    "        value_col_idx = self.time_series.columns.get_loc(self.value_col)\n",
    "        # iterate over the anomalies\n",
    "        for i in range(self.total_injections):\n",
    "            start_idx = self.start_indices[i]\n",
    "            duration = self.durations[i]\n",
    "            magnitude = self.magnitudes[i]\n",
    "            end_idx = start_idx + duration\n",
    "            # inject the anomaly\n",
    "            self.time_series = self.anomaly_generator.inject_anomaly(self.time_series, value_col_idx, self.anomaly, start_idx, end_idx, duration, magnitude)\n",
    "        # cap values exceeding obvious min and max\n",
    "        self.time_series[self.value_col] = np.clip(self.time_series[self.value_col], self.obvious_min, self.obvious_max)\n",
    "        \n",
    "        return self.time_series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualize anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(sensor_name, period_type, dry_wet_periods, buffer=None):\n",
    "\n",
    "    # get the start and end times\n",
    "    start = dry_wet_periods[sensor_name][period_type]['start']\n",
    "    end = dry_wet_periods[sensor_name][period_type]['end']\n",
    "\n",
    "    if buffer == \"day\":\n",
    "        # round start to start of day and end to end of day\n",
    "        start = start.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        # end is then the next day\n",
    "        end = start + timedelta(days=1)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # load the event data\n",
    "    rain_5425, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5425\", starttime=start, endtime=end, complete_range=True, verbose=True)\n",
    "    rain_5427, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5427\", starttime=start, endtime=end, complete_range=True, verbose=True)\n",
    "    sensor_data, _, _, _ = load_dataframe_from_HDF5(data_file_path, f\"single_series/sewer_data/{sensor_name}/clean\", starttime=start, endtime=end, complete_range=True, verbose=True)\n",
    "\n",
    "\n",
    "    data_dict = {\n",
    "        '5425': rain_5425,\n",
    "        '5427': rain_5427,\n",
    "        'original': sensor_data,\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "    }\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n",
    "def insert_anomaly(data_dict, anomaly_config, anomaly, sensor_scale, obvious_min, obvious_max, seed, center=False):\n",
    "    # create an anomaly handler\n",
    "    n_obs = data_dict['original'].shape[0]\n",
    "    anomaly_handler = AnomalyHandler(anomaly_config, anomaly, 'value', n_obs, sensor_scale, obvious_min, obvious_max, seed)\n",
    "    \n",
    "    # handling these example cases\n",
    "    # these can have multiple injections\n",
    "    if center:\n",
    "        # set start index to be in the middle of the time series\n",
    "        start_idx = n_obs*2 // 5\n",
    "        anomaly_handler.set_injection_start(start_idx)\n",
    "    else:\n",
    "        anomaly_handler.initialize_injections()\n",
    "\n",
    "    polluted_sensor = anomaly_handler.inject_anomalies(data_dict['original'])\n",
    "    data_dict['polluted'] = polluted_sensor\n",
    "    data_dict['indicator_dict'] = {\n",
    "        'indicator': anomaly_handler.get_indicator(),\n",
    "        'colormap': {\n",
    "            0: 'none',\n",
    "            1: 'firebrick',\n",
    "        }\n",
    "    }\n",
    "    return data_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rain(ax, title, data_dict, marker, linewidth=1):\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ### Visualize rain data\n",
    "    for rain_gauge, rain_color in zip(['5427', '5425'], ['purple', 'darkblue']):\n",
    "        ax.plot(data_dict[rain_gauge].index, data_dict[rain_gauge].value, \n",
    "            label=f'Rain gauge {rain_gauge}', color=rain_color,\n",
    "            linewidth=linewidth, linestyle='-', \n",
    "            marker=marker, markersize=1, alpha=1)\n",
    "        ax.set_ylabel('Rain (mm)')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.set_xticks([])\n",
    "        # set y limits based on 0 and max wrt both\n",
    "        ax.set_ylim(-1, data_dict['max_rain'])  \n",
    "        ax.set_xlim(data_dict['start'], data_dict['end'])\n",
    "    return ax\n",
    "\n",
    "\n",
    "def visualize_injected_synthetics(ax, title, data_dict, unit, marker):\n",
    "\n",
    "    # visualize the sensor data; severities\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.plot(data_dict['original'].index, data_dict['original'].value, \n",
    "        label='Original data', color='grey', \n",
    "        linewidth=1, linestyle='-', \n",
    "        marker='', markersize=1, alpha=1)\n",
    "\n",
    "    ax.plot(data_dict['polluted'].index, data_dict['polluted'].value, \n",
    "        label='Erroneous data', color='grey', \n",
    "        linewidth=2, linestyle='-', \n",
    "        marker=marker, markersize=2, alpha=1)\n",
    "    # visualzie error span\n",
    "    ax = visualize_error_span(ax, data_dict['indicator_dict'], data_dict['start'], data_dict['end'], adjust='full-point')\n",
    "    ax.set_xlim(data_dict['start'], data_dict['end'])\n",
    "    ax.set_ylabel(unit)\n",
    "    ax.legend()\n",
    "    ax.set_xticks([])\n",
    "    return ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder\n",
    "examples_save_folder = figure_save_folder / 'examples'\n",
    "examples_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "anomalies = list(synthetic_config['anomalies'].keys())\n",
    "severity = \"medium\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:22<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sensor_name in tqdm(natural_sensor_order, total=len(natural_sensor_order)):\n",
    "    if sensor_name != 'G80F11B_Level1':\n",
    "        continue\n",
    "\n",
    "    # create a folder for the sensor\n",
    "    sensor_save_folder = examples_save_folder / sensor_name\n",
    "    sensor_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    # extract meta\n",
    "    sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "    sensor_meta = metadata[metadata['IdMeasurement'] == sensor_name]\n",
    "    unit = sensor_meta['UnitAlias'].values[0]\n",
    "    obvious_min = sensor_meta['obvious_min'].values[0]\n",
    "    obvious_max = sensor_meta['obvious_max'].values[0]  \n",
    "\n",
    "\n",
    "    # plot a short and long period\n",
    "    #for buffer in [0, 1080]:\n",
    "    for buffer in ['period', 'day']:\n",
    "        marker = \"o\" if buffer == 'period' else \"\"\n",
    "\n",
    "        dry_data_dict = get_data_dict(sensor_name, 'dry_period', dry_wet_periods, buffer=buffer)\n",
    "        wet_data_dict = get_data_dict(sensor_name, 'rain_event', dry_wet_periods, buffer=buffer)\n",
    "\n",
    "        abs_max_rain = max([dry_data_dict['5425'].value.max(), dry_data_dict['5427'].value.max(), wet_data_dict['5425'].value.max(), wet_data_dict['5427'].value.max()]) + 1\n",
    "        dry_data_dict['max_rain'] = abs_max_rain    \n",
    "        wet_data_dict['max_rain'] = abs_max_rain\n",
    "\n",
    "        ### Visualizing all anomalies in one\n",
    "        fig, axs = plt.subplots(1+len(anomalies), 2, figsize=(14, 14), sharey=\"row\")\n",
    "        # set main title\n",
    "        fig.suptitle(f\"{sensor_name}\\n{buffer.capitalize()}\", fontsize=24)\n",
    "        axs[0, 0] = visualize_rain(axs[0, 0], 'Dry period', dry_data_dict, marker)\n",
    "        axs[0, 1] = visualize_rain(axs[0, 1], 'Rain event', wet_data_dict, marker)\n",
    "        # iterate over the anomalies\n",
    "        for i, anomaly in enumerate(anomalies):\n",
    "            anomaly_config = synthetic_config['anomalies'][anomaly][severity]\n",
    "\n",
    "            dry_data_dict = insert_anomaly(dry_data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=True)\n",
    "            wet_data_dict = insert_anomaly(wet_data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=True)\n",
    "            \n",
    "            # Visualize the injected anomalies\n",
    "            sensor_title = f\"{anomaly.capitalize()}\"\n",
    "            axs[i+1, 0] = visualize_injected_synthetics(axs[i+1, 0], sensor_title, dry_data_dict, unit, marker)\n",
    "            axs[i+1, 1] = visualize_injected_synthetics(axs[i+1, 1], sensor_title, wet_data_dict, unit, marker)\n",
    "            \n",
    "\n",
    "        axs[-1, 0] = set_meaningful_xticks(axs[-1, 0], dry_data_dict['start'], dry_data_dict['end'])\n",
    "        axs[-1, 1] = set_meaningful_xticks(axs[-1, 1], wet_data_dict['start'], wet_data_dict['end'])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # # save the figure\n",
    "        fig.savefig(sensor_save_folder / f\"all_{buffer}.png\", dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "        continue\n",
    "\n",
    "\n",
    "        ### Visualizing each anomaly\n",
    "        # iterate over the anomalies\n",
    "        for anomaly in anomalies:\n",
    "            anomaly_config = synthetic_config['anomalies'][anomaly][severity]\n",
    "            # using standard deviation as the range\n",
    "            #sensor_range = sensor_ranges[sensor_name]['clean']['std']\n",
    "            sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "\n",
    "\n",
    "            dry_data_dict = insert_anomaly(dry_data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=True)\n",
    "            wet_data_dict = insert_anomaly(wet_data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=True)\n",
    "\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(16, 6), sharey=\"row\", height_ratios=[1, 2])\n",
    "            # set main title\n",
    "            fig.suptitle(f\"{sensor_name}\\n{anomaly.capitalize()}\", fontsize=24)\n",
    "            sensor_title = f\"{severity.capitalize()} severity\"\n",
    "            axs[0, 0] = visualize_rain(axs[0, 0], 'Dry period', dry_data_dict)\n",
    "            axs[1, 0] = visualize_injected_synthetics(axs[1, 0], sensor_title, dry_data_dict, unit)\n",
    "            axs[-1, 0] = set_meaningful_xticks(axs[-1, 0], dry_data_dict['start'], dry_data_dict['end'])\n",
    "\n",
    "            axs[0, 1] = visualize_rain(axs[0, 1], 'Rain event', wet_data_dict)\n",
    "            axs[1, 1] = visualize_injected_synthetics(axs[1, 1], sensor_title, wet_data_dict, unit)\n",
    "            axs[-1, 1] = set_meaningful_xticks(axs[-1, 1], wet_data_dict['start'], wet_data_dict['end'])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            # save the figure\n",
    "            fig.savefig(sensor_save_folder / f\"{buffer}_{anomaly}.png\", dpi=150)\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize full timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_name = 'G80F11B_Level1'\n",
    "unit = metadata[metadata['IdMeasurement'] == sensor_name]['UnitAlias'].values[0]\n",
    "\n",
    "# create a folder for the sensor\n",
    "sensor_save_folder = examples_save_folder / sensor_name\n",
    "sensor_save_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# extract meta\n",
    "sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "sensor_meta = metadata[metadata['IdMeasurement'] == sensor_name]\n",
    "unit = sensor_meta['UnitAlias'].values[0]\n",
    "obvious_min = sensor_meta['obvious_min'].values[0]\n",
    "obvious_max = sensor_meta['obvious_max'].values[0]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full data\n",
    "sensor_data, _, _, _ = load_dataframe_from_HDF5(data_file_path, f\"single_series/sewer_data/{sensor_name}/clean\")\n",
    "\n",
    "start, end = sensor_data.index[0], sensor_data.index[-1]\n",
    "\n",
    "# load the event data\n",
    "rain_5425, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5425\", starttime=start, endtime=end, complete_range=True)\n",
    "rain_5427, _, _, _ = load_dataframe_from_HDF5(data_file_path, \"single_series/rain_gauge_data/5427\", starttime=start, endtime=end, complete_range=True)\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    '5425': rain_5425,\n",
    "    '5427': rain_5427,\n",
    "    'original': sensor_data,\n",
    "    'start': start,\n",
    "    'end': end,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_injected_synthetics(ax, title, data_dict, unit, marker):\n",
    "\n",
    "    # visualize the sensor data; severities\n",
    "    ax.set_title(title, fontsize=20)\n",
    "\n",
    "    ax.plot(data_dict['polluted'].index, data_dict['polluted'].value, \n",
    "        label='Erroneous data', color='grey', \n",
    "        linewidth=0.3, linestyle='-', \n",
    "        marker=marker, markersize=1, alpha=1)\n",
    "    # visualzie error span\n",
    "    ax = visualize_error_span(ax, data_dict['indicator_dict'], data_dict['start'], data_dict['end'], adjust=60*3, alpha=0.5)\n",
    "    ax.set_xlim(data_dict['start'], data_dict['end'])\n",
    "    ax.set_ylabel(unit)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xticks([])\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain done\n",
      "spike done: 0.00010136105958880849\n",
      "noise done: 0.0020669296481099298\n",
      "frozen done: 0.005003265498053763\n",
      "offset done: 0.020494788265106194\n",
      "drift done: 0.020130097442461924\n"
     ]
    }
   ],
   "source": [
    "marker = \"\"\n",
    "\n",
    "abs_max_rain = max([data_dict['5425'].value.max(), data_dict['5427'].value.max()]) + 1\n",
    "data_dict['max_rain'] = abs_max_rain    \n",
    "\n",
    "### Visualizing all anomalies in one\n",
    "fig, axs = plt.subplots(1+len(anomalies), 1, figsize=(14, 14), sharey=\"row\", dpi=150)\n",
    "# set main title\n",
    "fig.suptitle(f\"{sensor_name}\\n{' '}\", fontsize=24)\n",
    "axs[0] = visualize_rain(axs[0], 'Rain', data_dict, marker, linewidth=0.3)\n",
    "print('Rain done')\n",
    "# iterate over the anomalies\n",
    "for i, anomaly in enumerate(anomalies):\n",
    "    anomaly_config = synthetic_config['anomalies'][anomaly][severity]\n",
    "    sensor_range = sensor_ranges[sensor_name]['clean']['range']\n",
    "\n",
    "    data_dict = insert_anomaly(data_dict, anomaly_config, anomaly, sensor_range, obvious_min, obvious_max, seed, center=False)\n",
    "    \n",
    "    # Visualize the injected anomalies\n",
    "    sensor_title = f\"{anomaly.capitalize()}\"\n",
    "    axs[i+1] = visualize_injected_synthetics(axs[i+1], sensor_title, data_dict, unit, marker)\n",
    "\n",
    "    print(f\"{anomaly} done: {data_dict['indicator_dict']['indicator'].sum() / data_dict['indicator_dict']['indicator'].shape[0]}\")\n",
    "\n",
    "    \n",
    "\n",
    "axs[-1] = set_meaningful_xticks(axs[-1], data_dict['start'], data_dict['end'])\n",
    "plt.tight_layout()\n",
    "# # save the figure\n",
    "fig.savefig(sensor_save_folder / f\"all_complete.png\", dpi=150)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Todo\n",
    "\n",
    "- compare this real errors?\n",
    "- some synthetic data summary?\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
